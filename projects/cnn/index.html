<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> numpy cnn | Alvin Chen </title> <meta name="author" content="Alvin Chen"> <meta name="description" content="A Convolutional Neural Network implemented in NumPy with modular layers."> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/favicon.ico?c6fddee0e29e79c86a7dd97fa38f204b"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://alvin-pc-chen.github.io/projects/cnn/"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js?0afe9f0ae161375728f7bcc5eb5b4ab4"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Alvin </span> Chen </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item active"> <a class="nav-link" href="/projects/">projects <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">repositories </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-solid fa-moon"></i> <i class="fa-solid fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">numpy cnn</h1> <p class="post-description">A Convolutional Neural Network implemented in NumPy with modular layers.</p> </header> <article> <div class="row justify-content-md-center"> <div class="col-sm-auto mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/projects/cnn_title_card-480.webp 480w,/assets/img/projects/cnn_title_card-800.webp 800w,/assets/img/projects/cnn_title_card-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/projects/cnn_title_card.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="title card" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p>Neural network implementations are more easily accessible now than ever before, abstracting away complexities like gradients, activation layers, and training algorithms. Building neural networks layer by layer from scratch helped me develop a deeper understanding of the structural capabilities and shortcomings of these models. In this project, I implement modular layers for a scalable <strong><a href="https://www.geeksforgeeks.org/introduction-convolution-neural-network/" rel="external nofollow noopener" target="_blank">Convolutional Neural Network</a></strong> using numpy and train it on the <strong><a href="https://github.com/zalandoresearch/fashion-mnist" rel="external nofollow noopener" target="_blank">Fashion MNIST</a></strong> dataset to perform a simple classification task.</p> <p>My implementation is both <strong>flexible</strong>, with variable input, output, kernel, pool sizes, and <strong>modular</strong>, with separately implemented layers that can be arbitrarily stacked. I achieve an accuracy comparable to a TensorFlow model using the same structure (<code class="language-plaintext highlighter-rouge">NumPy Accuracy = 0.891</code> vs <code class="language-plaintext highlighter-rouge">Keras Accuracy = 0.887</code>), albeit with much slower runtime.</p> <p>This writeup goes in much more detail, but code for this project can be found <strong><a href="https://github.com/alvin-pc-chen/cnn-from-scratch" rel="external nofollow noopener" target="_blank">here</a></strong>.</p> <h2 id="table-of-contents">Table of Contents</h2> <ol> <li><a href="#why-convolutional-neural-networks">Why Convolutional Neural Networks?</a></li> <li><a href="#the-fashion-mnist-dataset">Fashion MNIST</a></li> <li><a href="#the-convolutional-layer">The Convolutional Layer</a></li> <li><a href="#relu-activation">ReLU Activation</a></li> <li><a href="#the-pooling-layer">The Pooling Layer</a></li> <li><a href="#the-feedforward-layer">The Feedforward Layer</a></li> <li><a href="#the-output-layer">The Output Layer</a></li> <li><a href="#model-architecture">Architecture</a></li> <li><a href="#benchmarking-with-keras">Benchmark</a></li> <li><a href="#sources">Sources</a></li> </ol> <h2 id="why-convolutional-neural-networks">Why Convolutional Neural Networks?</h2> <p>Neural networks are made up of three layers: the input, hidden, and output layers. The input layer takes some data (text, images, stock prices, etc.) and processes it for the hidden layer. The hidden layer is where the magic happens: weights and activation fucntions are consecutively applied to the data to extract features necessary to the task at hand. Depending on the task, all kinds of structural complexities can be added to the hidden layer to improve model performance. Finally, these features are passed to the output layer which also uses weights and an activation function to return the desired output. For image classification, this will be a vector whose values represent to the probability that the input image belongs to each class.</p> <div class="row justify-content-md-center"> <div class="col-sm-auto mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/projects/cnn_fnn-480.webp 480w,/assets/img/projects/cnn_fnn-800.webp 800w,/assets/img/projects/cnn_fnn-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/projects/cnn_fnn.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="simple feedforward neural network" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p>The <strong><a href="https://www.geeksforgeeks.org/understanding-multi-layer-feed-forward-networks/" rel="external nofollow noopener" target="_blank">Feedforward Network</a></strong> (FFN) is a foundational neural network that perfectly illustrates this concept. The network takes a 1-dimensional vector as input, in this case the pixels of an image, and passes it through a number of hidden layers each comprised of a weight matrix and an activation function. The FNN is a <strong>naive</strong> approach, meaning that almost no assumptions are made about the input data. Since all input data is flattened into a 1-dimensional vector for the network, the weight matrices do all of the heavy lifting. As you can imagine, FNN’s perform poorly on complex tasks and are far outclassed by other models. The reason for this touches on a fundamental concept in model design: <strong>gains in predictive power rely on making stronger assumptions about the data and incorporating them into the architecture of the model</strong>. Doing so reduces the search space, which removes predictions we know are impossible and improves training efficiency.</p> <p>Enter the <strong><a href="https://www.geeksforgeeks.org/introduction-convolution-neural-network/" rel="external nofollow noopener" target="_blank">Convolutional Neural Network</a></strong> (CNN), which makes two core assumptions:</p> <ol> <li>Features can be captured by looking at regions of neighboring pixels;</li> <li>Not all pixels are necessary for image classification.</li> </ol> <p>The CNN encodes the first assumption through <code class="language-plaintext highlighter-rouge">convolution</code>, which involves sliding a <code class="language-plaintext highlighter-rouge">kernel</code> across the input matrix to capture important features. After training, the weights of the kernels highlight parts of the image pertinent to the classification task by combining information within regions of the input matrix. If neighboring pixels within a region indeed contain useful information, the kernels will convert them to better features for classification. The second assumption is encoded through <code class="language-plaintext highlighter-rouge">pooling</code>, which systematically reduces matrix size and preserves only the most significant element in each region. The resulting matrix is then passed to a <code class="language-plaintext highlighter-rouge">feedforward layer</code> which can be simply a softmax classification layer or even an entire FNN.</p> <p>Let’s compare these two models classifying a <code class="language-plaintext highlighter-rouge">28x28</code> pixel RGB image. The FNN converts it into a <code class="language-plaintext highlighter-rouge">28*28*3 = 2352x1</code> array, so the network cannot infer that the first three elements represent the same pixel or that the next three elements represent a neighboring pixel. As we will see, a simple CNN will produce a <code class="language-plaintext highlighter-rouge">14x14x6</code> matrix that much better represents the features of the input image; flattening to a <code class="language-plaintext highlighter-rouge">1176x1</code> array presents the <code class="language-plaintext highlighter-rouge">output layer</code> with half as many inputs containing much more significant information.</p> <h2 id="the-fashion-mnist-dataset">The Fashion MNIST Dataset</h2> <p>The <strong><a href="https://github.com/zalandoresearch/fashion-mnist" rel="external nofollow noopener" target="_blank">Fashion MNIST</a></strong> dataset was modeled on the original <strong><a href="http://yann.lecun.com/exdb/mnist/" rel="external nofollow noopener" target="_blank">MNIST</a></strong> database of handwritten digits to provide a more challenging image classification task. While Fashion MNIST is also a set of 70k (60k train and 10k test) black and white <code class="language-plaintext highlighter-rouge">28x28</code> pixel images, the classes are much more abstract than the original MNIST. Each image is an article of clothing belonging to one of 10 classes: “T-shirt/top”, “Trouser”, “Pullover”, “Dress”, “Coat”, “Sandal”, “Shirt”, “Sneaker”, “Bag”, and “Ankle boot”:</p> <div class="row justify-content-md-center"> <div class="col-sm-auto mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/projects/cnn_sample-480.webp 480w,/assets/img/projects/cnn_sample-800.webp 800w,/assets/img/projects/cnn_sample-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/projects/cnn_sample.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="sample images" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> A sample of images in Fashion MNIST. </div> <h2 id="the-convolutional-layer">The Convolutional Layer</h2> <div class="row justify-content-md-center"> <div class="col-md-auto mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/projects/cnn_cross-correlate-480.webp 480w,/assets/img/projects/cnn_cross-correlate-800.webp 800w,/assets/img/projects/cnn_cross-correlate-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/projects/cnn_cross-correlate.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="cross-correlation" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> The cross-correlation mechanism. </div> <p>The core of the CNN is <code class="language-plaintext highlighter-rouge">convlution</code>: square kernels with odd-numbered length are multiplied elementwise on each region in the image and summed up. Strictly speaking this is <strong><a href="https://towardsdatascience.com/convolution-vs-cross-correlation-81ec4a0ec253" rel="external nofollow noopener" target="_blank">cross-correlation</a></strong>; in convlution the kernel has to be rotated by <code class="language-plaintext highlighter-rouge">180°</code>, as seen below. The kernel is iterated across the image in <code class="language-plaintext highlighter-rouge">strides</code> and outputs are positionally combined to form a 2D matrix.</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr>
<td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
</pre></td> <td class="code"><pre><span class="k">class</span> <span class="nc">ConvLayer</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">input_shape</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">num_kernels</span><span class="o">=</span><span class="mi">6</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>
        <span class="c1"># Get input dimensions
</span>        <span class="n">input_depth</span><span class="p">,</span> <span class="n">input_height</span><span class="p">,</span> <span class="n">input_width</span> <span class="o">=</span> <span class="n">input_shape</span>
        <span class="n">self</span><span class="p">.</span><span class="n">d</span> <span class="o">=</span> <span class="n">input_depth</span>
        <span class="n">self</span><span class="p">.</span><span class="n">h</span> <span class="o">=</span> <span class="n">input_height</span> <span class="o">+</span> <span class="n">kernel_size</span> <span class="o">-</span> <span class="mi">1</span>
        <span class="n">self</span><span class="p">.</span><span class="n">w</span> <span class="o">=</span> <span class="n">input_width</span> <span class="o">+</span> <span class="n">kernel_size</span> <span class="o">-</span> <span class="mi">1</span>
        <span class="n">self</span><span class="p">.</span><span class="n">input_shape</span> <span class="o">=</span> <span class="n">input_shape</span>
        <span class="c1"># Initialize kernels and bias
</span>        <span class="n">self</span><span class="p">.</span><span class="n">padding</span> <span class="o">=</span> <span class="n">padding</span>
        <span class="n">self</span><span class="p">.</span><span class="n">num_kernels</span> <span class="o">=</span> <span class="n">num_kernels</span>
        <span class="n">self</span><span class="p">.</span><span class="n">kernel_size</span> <span class="o">=</span> <span class="n">kernel_size</span>
        <span class="n">self</span><span class="p">.</span><span class="n">pad_size</span> <span class="o">=</span> <span class="n">kernel_size</span> <span class="o">//</span> <span class="mi">2</span>
        <span class="n">self</span><span class="p">.</span><span class="n">kernel_shape</span> <span class="o">=</span> <span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">num_kernels</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">d</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">kernel_size</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">kernel_size</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">bias_shape</span> <span class="o">=</span> <span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">num_kernels</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">h</span> <span class="o">-</span> <span class="n">self</span><span class="p">.</span><span class="n">kernel_size</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">w</span> <span class="o">-</span> <span class="n">self</span><span class="p">.</span><span class="n">kernel_size</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
        <span class="c1"># Dividing mimics Xavier Initialization and reduces variance
</span>        <span class="n">self</span><span class="p">.</span><span class="n">kernels</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="o">*</span><span class="n">self</span><span class="p">.</span><span class="n">kernel_shape</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">kernel_size</span> <span class="o">*</span> <span class="n">self</span><span class="p">.</span><span class="n">kernel_size</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">bias</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="o">*</span><span class="n">self</span><span class="p">.</span><span class="n">bias_shape</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">h</span> <span class="o">*</span> <span class="n">self</span><span class="p">.</span><span class="n">w</span><span class="p">)</span>
</pre></td> </tr></tbody></table></code></pre></figure> <p>Notice that the resulting array will be smaller than the original image. In order to produce arrays of equal size, this implementation <code class="language-plaintext highlighter-rouge">pads</code> the array with borders of zeroes (border size <code class="language-plaintext highlighter-rouge">2</code> for the <code class="language-plaintext highlighter-rouge">5x5</code> kernel), called <code class="language-plaintext highlighter-rouge">full convolution</code> as opposed to <code class="language-plaintext highlighter-rouge">valid convlution</code>. In this implementation, kernel weights and biases are customizeable by size and number.</p> <h3 id="forward">Forward</h3> <figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr>
<td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
</pre></td> <td class="code"><pre><span class="k">class</span> <span class="nc">ConvLayer</span><span class="p">:</span>
    <span class="c1">#...
</span>    <span class="k">def</span> <span class="nf">iter_regions</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">image</span><span class="p">):</span>
        <span class="sh">"""</span><span class="s">
        Generates all possible (kernel_size x kernel_size) image regions (prepadded)
        </span><span class="sh">"""</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">h</span> <span class="o">-</span> <span class="n">self</span><span class="p">.</span><span class="n">kernel_size</span> <span class="o">+</span> <span class="mi">1</span><span class="p">):</span>
            <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">w</span> <span class="o">-</span> <span class="n">self</span><span class="p">.</span><span class="n">kernel_size</span> <span class="o">+</span> <span class="mi">1</span><span class="p">):</span>
                <span class="n">im_region</span> <span class="o">=</span> <span class="n">image</span><span class="p">[:,</span> <span class="n">i</span><span class="p">:(</span><span class="n">i</span> <span class="o">+</span> <span class="n">self</span><span class="p">.</span><span class="n">kernel_size</span><span class="p">),</span> <span class="n">j</span><span class="p">:(</span><span class="n">j</span> <span class="o">+</span> <span class="n">self</span><span class="p">.</span><span class="n">kernel_size</span><span class="p">)]</span>
                <span class="k">yield</span> <span class="n">im_region</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span> <span class="n">j</span>
    
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">):</span>
        <span class="sh">"""</span><span class="s">
        Pad input, get regions, and perform full cross correlation with kernels
        </span><span class="sh">"""</span>
        <span class="n">padded</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">pad</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="p">((</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">),</span> <span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">pad_size</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">pad_size</span><span class="p">),</span> <span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">pad_size</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">pad_size</span><span class="p">)),</span> <span class="n">mode</span><span class="o">=</span><span class="sh">"</span><span class="s">constant</span><span class="sh">"</span><span class="p">,</span> <span class="n">constant_values</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">padding</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">prev_input</span> <span class="o">=</span> <span class="n">padded</span> <span class="c1"># Save for backpropagation
</span>        <span class="n">self</span><span class="p">.</span><span class="n">output</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">copy</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">bias</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">im_region</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span> <span class="n">j</span> <span class="ow">in</span> <span class="n">self</span><span class="p">.</span><span class="nf">iter_regions</span><span class="p">(</span><span class="n">padded</span><span class="p">):</span>
            <span class="n">self</span><span class="p">.</span><span class="n">output</span><span class="p">[:,</span> <span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">]</span> <span class="o">+=</span> <span class="n">np</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">im_region</span> <span class="o">*</span> <span class="n">self</span><span class="p">.</span><span class="n">kernels</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
        <span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="n">output</span>
</pre></td> </tr></tbody></table></code></pre></figure> <p>The <code class="language-plaintext highlighter-rouge">forward()</code> function is straightforward:</p> <ol> <li>Pad the input matrix according to kernel size;</li> <li>Using the <code class="language-plaintext highlighter-rouge">iter_regions()</code> method, get all regions that need to be cross-correlated with the kernel;</li> <li>Perform the cross correlation by multiplying elementwise (numpy’s multiply function does this by default) and then sum up elements on all axes.</li> </ol> <p>Since we generally work with more than one kernel and input matrices are often 3D, we can stack kernels into a 4D matrix that can be easily multiplied with the 3D input array. The regions will be multiplied along the first dimension of the kernel array and summed up along the other three dimensions which will generate a 1D vector representing the output of each kernel on the region.</p> <h3 id="backpropagation">Backpropagation</h3> <figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr>
<td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
</pre></td> <td class="code"><pre><span class="k">class</span> <span class="nc">ConvLayer</span><span class="p">:</span>
    <span class="c1">#...
</span>    <span class="k">def</span> <span class="nf">backprop</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">d_L_d_out</span><span class="p">,</span> <span class="n">learn_rate</span><span class="p">):</span>
        <span class="sh">"""</span><span class="s">
        Update kernels and bias, and return input gradient
        </span><span class="sh">"""</span>
        <span class="c1"># Cross correlation for kernel gradient
</span>        <span class="n">d_L_d_kernels</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">zeros</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">kernels</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">im_region</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span> <span class="n">j</span> <span class="ow">in</span> <span class="n">self</span><span class="p">.</span><span class="nf">iter_regions</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">prev_input</span><span class="p">):</span>
            <span class="k">for</span> <span class="n">f</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">num_kernels</span><span class="p">):</span>
                <span class="n">d_L_d_kernels</span><span class="p">[</span><span class="n">f</span><span class="p">]</span> <span class="o">+=</span> <span class="n">d_L_d_out</span><span class="p">[</span><span class="n">f</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">]</span> <span class="o">*</span> <span class="n">im_region</span> 
        <span class="c1"># Full convolution for input gradient
</span>        <span class="n">d_L_d_input</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">zeros</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">input_shape</span><span class="p">)</span>
        <span class="n">pad_out</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">pad</span><span class="p">(</span><span class="n">d_L_d_out</span><span class="p">,</span> <span class="p">((</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">),</span> <span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">pad_size</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">pad_size</span><span class="p">),</span> <span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">pad_size</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">pad_size</span><span class="p">)),</span> <span class="n">mode</span><span class="o">=</span><span class="sh">"</span><span class="s">constant</span><span class="sh">"</span><span class="p">,</span> <span class="n">constant_values</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">conv_kernels</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">rot90</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">moveaxis</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">kernels</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="mi">2</span><span class="p">,</span> <span class="n">axes</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
        <span class="k">for</span> <span class="n">im_region2</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span> <span class="n">j</span> <span class="ow">in</span> <span class="n">self</span><span class="p">.</span><span class="nf">iter_regions</span><span class="p">(</span><span class="n">pad_out</span><span class="p">):</span>
            <span class="k">for</span> <span class="n">d</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">d</span><span class="p">):</span>
                <span class="n">d_L_d_input</span><span class="p">[</span><span class="n">d</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">]</span> <span class="o">+=</span> <span class="n">np</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">im_region2</span> <span class="o">*</span> <span class="n">conv_kernels</span><span class="p">[</span><span class="n">d</span><span class="p">])</span>
        <span class="c1"># Adjust by learn rate
</span>        <span class="n">self</span><span class="p">.</span><span class="n">bias</span> <span class="o">-=</span> <span class="n">learn_rate</span> <span class="o">*</span> <span class="n">d_L_d_out</span>
        <span class="n">self</span><span class="p">.</span><span class="n">kernels</span> <span class="o">-=</span> <span class="n">learn_rate</span> <span class="o">*</span> <span class="n">d_L_d_kernels</span>
        <span class="k">return</span> <span class="n">d_L_d_input</span>
</pre></td> </tr></tbody></table></code></pre></figure> <p>The backpropagation for the <code class="language-plaintext highlighter-rouge">convolutional layer</code> is much more complicated, especially since an <code class="language-plaintext highlighter-rouge">input gradient</code> must also be calculated to backpropagate to lower layers. Terminology for backpropagation may be tricky here: the <code class="language-plaintext highlighter-rouge">input gradient</code> is the gradient of the input matrix during the forward phase. For the first layer, that matrix represents the input image. The <code class="language-plaintext highlighter-rouge">output gradient</code> is the input to the <code class="language-plaintext highlighter-rouge">backprop()</code> function, which for the <code class="language-plaintext highlighter-rouge">output layer</code> is the initial gradient calculated with the gold standard label. The bias gradient is easiest to resolve: since it is the base of the output, it should simply be adjusted by the output gradient.</p> <p>To understand backpropagating the <code class="language-plaintext highlighter-rouge">kernel weights</code>, consider that each kernel is iterated across the input matrix and apply to all regions equally, which means that each region of the gradient should have some effect on the kernel. The kernel gradient is therefore the sum of the product between the output gradient and the previous input matrix for each region of kernel size.</p> <div class="row justify-content-md-center"> <div class="col-md-auto mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/projects/cnn_convolution-480.webp 480w,/assets/img/projects/cnn_convolution-800.webp 800w,/assets/img/projects/cnn_convolution-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/projects/cnn_convolution.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="convolution" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> The actual mechanism for convolution. </div> <p>The <code class="language-plaintext highlighter-rouge">input gradient</code> is where we encounter <code class="language-plaintext highlighter-rouge">convolution</code>. Without going into too much mathematical detail, consider that each input element has a different and unequal effect on the output: corner elements will only contribute to the one corner element in the output whereas edges and central elements will contribute to multiple output elements. This same pattern occurs when calculating the derivative with respect to the input elements, since output elements that were not calculated with the input element will have a derivative of <code class="language-plaintext highlighter-rouge">0</code> with respect to that input element. The key difference is that each output element is affected by the opposite kernel element, which means that the kernel must be rotated <code class="language-plaintext highlighter-rouge">180°</code> (precisely as in convolution) in order to calculate the gradient for the input element. In order to produce this pattern, a <code class="language-plaintext highlighter-rouge">full convolution</code> must be used, even for models that use <code class="language-plaintext highlighter-rouge">valid cross-correlation</code> in the <code class="language-plaintext highlighter-rouge">forward()</code> portion.</p> <h2 id="relu-activation">ReLU Activation</h2> <div class="row justify-content-md-center"> <div class="col-md-auto mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/projects/cnn_relu-480.webp 480w,/assets/img/projects/cnn_relu-800.webp 800w,/assets/img/projects/cnn_relu-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/projects/cnn_relu.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="relu" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p>At the core of the neural network is the <code class="language-plaintext highlighter-rouge">activation function</code>, which is applied to the outputs of the previous layer to introduce nonlinearity. Without activation, stacked layers remain linear and have the same predictive power as a single layer. The downside to the nonlinear function occurs in backpropagation, where certain outputs can result in disappearing gradients breaking the training loop. For this reason, many models prefer using <a href="https://machinelearningmastery.com/rectified-linear-activation-function-for-deep-learning-neural-networks/" rel="external nofollow noopener" target="_blank"><code class="language-plaintext highlighter-rouge">ReLU</code></a> as the activation function for non-terminal layers. <code class="language-plaintext highlighter-rouge">ReLU</code> behaves linearly for inputs greater than <code class="language-plaintext highlighter-rouge">0</code> which means its derivative is <code class="language-plaintext highlighter-rouge">1</code>, preventing disappearing gradients while maintaining nonlinearity. The derivative of <code class="language-plaintext highlighter-rouge">ReLU</code> is undefined at <code class="language-plaintext highlighter-rouge">0</code> conventionally we set it to <code class="language-plaintext highlighter-rouge">0</code>.</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr>
<td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
</pre></td> <td class="code"><pre><span class="k">class</span> <span class="nc">ReLU</span><span class="p">:</span>
    <span class="sh">"""</span><span class="s">
    Simple ReLU activation function
    </span><span class="sh">"""</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="k">pass</span>
    
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">):</span>
        <span class="n">self</span><span class="p">.</span><span class="n">prev_output</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">maximum</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="nb">input</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="n">prev_output</span>
    
    <span class="k">def</span> <span class="nf">backprop</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">d_L_d_out</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">d_L_d_out</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="nf">int64</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">prev_output</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">)</span>
</pre></td> </tr></tbody></table></code></pre></figure> <p>As seen in the code, the <code class="language-plaintext highlighter-rouge">ReLU</code> class implemented here is easy to use. Once we run <code class="language-plaintext highlighter-rouge">forward()</code> the function will take the shape of the <code class="language-plaintext highlighter-rouge">previous output</code>. The <code class="language-plaintext highlighter-rouge">backprop()</code> function is also straightforward, simply passing all outputs that were not zeroed out by <code class="language-plaintext highlighter-rouge">ReLU</code> and multiplying by the gradient.</p> <h2 id="the-pooling-layer">The Pooling Layer</h2> <div class="row justify-content-md-center"> <div class="col-md-auto mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/projects/cnn_pool-480.webp 480w,/assets/img/projects/cnn_pool-800.webp 800w,/assets/img/projects/cnn_pool-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/projects/cnn_pool.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="relu" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p>We’ve seen that kernels in the <code class="language-plaintext highlighter-rouge">convolutional layer</code> capture information from neighboring pixels, which means that many elements in the output array contain redundant information. While this is hardly an issue for the <code class="language-plaintext highlighter-rouge">28x28</code> Fashion MNIST images, computation can quickly get out of hand for multilayer networks processing images with thousands of pixels. <code class="language-plaintext highlighter-rouge">Pooling</code> presents a simple solution: run another square array across the output matrix and at each <code class="language-plaintext highlighter-rouge">stride</code> keep only the <code class="language-plaintext highlighter-rouge">max</code>, <code class="language-plaintext highlighter-rouge">min</code>, or <code class="language-plaintext highlighter-rouge">mean</code> value. <code class="language-plaintext highlighter-rouge">Pooling</code> only makes sense because of our second assumption that neighboring pixels contain redundant information, which is true of the <code class="language-plaintext highlighter-rouge">convolutional layer</code> output.</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr>
<td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
</pre></td> <td class="code"><pre><span class="k">class</span> <span class="nc">MaxPool</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">pool_size</span><span class="o">=</span><span class="mi">2</span><span class="p">):</span>
        <span class="n">self</span><span class="p">.</span><span class="n">size</span> <span class="o">=</span> <span class="n">pool_size</span>
    
    <span class="k">def</span> <span class="nf">iter_regions</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">image</span><span class="p">):</span>
        <span class="sh">"""</span><span class="s">
        Same as Conv layer, but with stride of pool_size
        </span><span class="sh">"""</span>
        <span class="n">_</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">w</span> <span class="o">=</span> <span class="n">image</span><span class="p">.</span><span class="n">shape</span>
        <span class="n">new_h</span> <span class="o">=</span> <span class="n">h</span> <span class="o">//</span> <span class="n">self</span><span class="p">.</span><span class="n">size</span>
        <span class="n">new_w</span> <span class="o">=</span> <span class="n">w</span> <span class="o">//</span> <span class="n">self</span><span class="p">.</span><span class="n">size</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">new_h</span><span class="p">):</span>
            <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">new_w</span><span class="p">):</span>
                <span class="n">im_region</span> <span class="o">=</span> <span class="n">image</span><span class="p">[:,</span> <span class="p">(</span><span class="n">i</span> <span class="o">*</span> <span class="n">self</span><span class="p">.</span><span class="n">size</span><span class="p">):(</span><span class="n">i</span> <span class="o">*</span> <span class="n">self</span><span class="p">.</span><span class="n">size</span> <span class="o">+</span> <span class="n">self</span><span class="p">.</span><span class="n">size</span><span class="p">),</span> <span class="p">(</span><span class="n">j</span> <span class="o">*</span> <span class="n">self</span><span class="p">.</span><span class="n">size</span><span class="p">):(</span><span class="n">j</span> <span class="o">*</span> <span class="n">self</span><span class="p">.</span><span class="n">size</span> <span class="o">+</span> <span class="n">self</span><span class="p">.</span><span class="n">size</span><span class="p">)]</span>
                <span class="k">yield</span> <span class="n">im_region</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span> <span class="n">j</span>
    
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">):</span>
        <span class="sh">"""</span><span class="s">
        Gets max value in each region
        </span><span class="sh">"""</span>
        <span class="n">self</span><span class="p">.</span><span class="n">prev_input</span> <span class="o">=</span> <span class="nb">input</span>
        <span class="n">num_kernels</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">w</span> <span class="o">=</span> <span class="nb">input</span><span class="p">.</span><span class="n">shape</span>
        <span class="n">output</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">zeros</span><span class="p">((</span><span class="n">num_kernels</span><span class="p">,</span> <span class="n">h</span> <span class="o">//</span> <span class="n">self</span><span class="p">.</span><span class="n">size</span><span class="p">,</span> <span class="n">w</span> <span class="o">//</span> <span class="n">self</span><span class="p">.</span><span class="n">size</span><span class="p">))</span>
        <span class="k">for</span> <span class="n">im_region</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span> <span class="n">j</span> <span class="ow">in</span> <span class="n">self</span><span class="p">.</span><span class="nf">iter_regions</span><span class="p">(</span><span class="nb">input</span><span class="p">):</span>
            <span class="n">output</span><span class="p">[:,</span> <span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">amax</span><span class="p">(</span><span class="n">im_region</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
        <span class="k">return</span> <span class="n">output</span>

    <span class="k">def</span> <span class="nf">backprop</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">d_L_d_out</span><span class="p">):</span>
        <span class="sh">"""</span><span class="s">
        Backpropagates gradient to input
        </span><span class="sh">"""</span>
        <span class="n">d_L_d_input</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">zeros</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">prev_input</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">im_region</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span> <span class="n">j</span> <span class="ow">in</span> <span class="n">self</span><span class="p">.</span><span class="nf">iter_regions</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">prev_input</span><span class="p">):</span>
            <span class="n">f</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">w</span> <span class="o">=</span> <span class="n">im_region</span><span class="p">.</span><span class="n">shape</span>
            <span class="n">amax</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">amax</span><span class="p">(</span><span class="n">im_region</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
            <span class="k">for</span> <span class="n">i2</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">h</span><span class="p">):</span>
                <span class="k">for</span> <span class="n">j2</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">w</span><span class="p">):</span>
                    <span class="k">for</span> <span class="n">f2</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">f</span><span class="p">):</span>
                        <span class="k">if</span> <span class="n">im_region</span><span class="p">[</span><span class="n">f2</span><span class="p">,</span> <span class="n">i2</span><span class="p">,</span> <span class="n">j2</span><span class="p">]</span> <span class="o">==</span> <span class="n">amax</span><span class="p">[</span><span class="n">f2</span><span class="p">]:</span>
                            <span class="n">d_L_d_input</span><span class="p">[</span><span class="n">f2</span><span class="p">,</span> <span class="n">i</span> <span class="o">*</span> <span class="n">self</span><span class="p">.</span><span class="n">size</span> <span class="o">+</span> <span class="n">i2</span><span class="p">,</span> <span class="n">j</span> <span class="o">*</span> <span class="n">self</span><span class="p">.</span><span class="n">size</span> <span class="o">+</span> <span class="n">j2</span><span class="p">]</span> <span class="o">=</span> <span class="n">d_L_d_out</span><span class="p">[</span><span class="n">f2</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">]</span>
        <span class="k">return</span> <span class="n">d_L_d_input</span>
</pre></td> </tr></tbody></table></code></pre></figure> <p>This implementation uses a <code class="language-plaintext highlighter-rouge">2x2 max pool</code>, but any value will evenly reduce the size of the output while keeping the most important information. The size of the <code class="language-plaintext highlighter-rouge">pool</code> should be balanced with the size of the <code class="language-plaintext highlighter-rouge">kernel</code> in the <code class="language-plaintext highlighter-rouge">convolutional layer</code>; At minimum, each <code class="language-plaintext highlighter-rouge">pooling layer</code> reduces the size of the input by a factor of <code class="language-plaintext highlighter-rouge">4</code>. Since no weights are learned, the <code class="language-plaintext highlighter-rouge">forward()</code> and <code class="language-plaintext highlighter-rouge">backprop()</code> is deterministic. In the forward pass, the input array is reduced by the method described. In the backpropagation portion, we simply place the gradient values in their respective positions in the input matrix and set all other values to zero.</p> <h2 id="the-feedforward-layer">The Feedforward Layer</h2> <div class="row justify-content-md-center"> <div class="col-sm-auto mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/projects/cnn_feedforward-480.webp 480w,/assets/img/projects/cnn_feedforward-800.webp 800w,/assets/img/projects/cnn_feedforward-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/projects/cnn_feedforward.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="feedforward" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p>We now have a working system for extracting features from input images using <code class="language-plaintext highlighter-rouge">convolution</code>, which leaves only the <code class="language-plaintext highlighter-rouge">classification</code> step left. For smaller models, a single <code class="language-plaintext highlighter-rouge">softmax</code> classification layer as seen below may be sufficient, but generally a full <code class="language-plaintext highlighter-rouge">feedforward network</code> is used for prediction. The <code class="language-plaintext highlighter-rouge">feedfoward layer</code> collapses the input matrix into a 1-dimensional array, with each element of the array representing a separate feature for the <code class="language-plaintext highlighter-rouge">feedforward network</code>.</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr>
<td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
</pre></td> <td class="code"><pre><span class="k">class</span> <span class="nc">FeedForward</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">input_size</span><span class="p">,</span> <span class="n">output_size</span><span class="p">):</span>
        <span class="n">self</span><span class="p">.</span><span class="n">weights</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="n">input_size</span><span class="p">,</span> <span class="n">output_size</span><span class="p">)</span> <span class="o">/</span> <span class="n">input_size</span>
        <span class="n">self</span><span class="p">.</span><span class="n">bias</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="n">output_size</span><span class="p">)</span> <span class="o">/</span> <span class="n">output_size</span>
    
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">):</span>
        <span class="sh">"""</span><span class="s">
        Multiply by weights and add bias
        </span><span class="sh">"""</span>
        <span class="n">self</span><span class="p">.</span><span class="n">prev_input_shape</span> <span class="o">=</span> <span class="nb">input</span><span class="p">.</span><span class="n">shape</span>
        <span class="nb">input</span> <span class="o">=</span> <span class="nb">input</span><span class="p">.</span><span class="nf">flatten</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">prev_input</span> <span class="o">=</span> <span class="nb">input</span>
        <span class="n">self</span><span class="p">.</span><span class="n">output</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">dot</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">weights</span><span class="p">)</span> <span class="o">+</span> <span class="n">self</span><span class="p">.</span><span class="n">bias</span>
        <span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="n">output</span>
    
    <span class="k">def</span> <span class="nf">backprop</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">d_L_d_out</span><span class="p">,</span> <span class="n">learn_rate</span><span class="p">):</span>
        <span class="sh">"""</span><span class="s">
        Update weights and bias, and return input gradient
        </span><span class="sh">"""</span>
        <span class="n">d_out_d_weights</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">prev_input</span>
        <span class="n">d_out_d_input</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">weights</span>
        <span class="n">d_L_d_weights</span> <span class="o">=</span> <span class="n">d_out_d_weights</span><span class="p">[</span><span class="n">np</span><span class="p">.</span><span class="n">newaxis</span><span class="p">].</span><span class="n">T</span> <span class="o">@</span> <span class="n">d_L_d_out</span><span class="p">[</span><span class="n">np</span><span class="p">.</span><span class="n">newaxis</span><span class="p">]</span>
        <span class="n">d_L_d_input</span> <span class="o">=</span> <span class="n">d_out_d_input</span> <span class="o">@</span> <span class="n">d_L_d_out</span>
        <span class="n">self</span><span class="p">.</span><span class="n">weights</span> <span class="o">-=</span> <span class="n">learn_rate</span> <span class="o">*</span> <span class="n">d_L_d_weights</span>
        <span class="n">self</span><span class="p">.</span><span class="n">bias</span> <span class="o">-=</span> <span class="n">learn_rate</span> <span class="o">*</span> <span class="n">d_L_d_out</span>
        <span class="k">return</span> <span class="n">d_L_d_input</span><span class="p">.</span><span class="nf">reshape</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">prev_input_shape</span><span class="p">)</span>
</pre></td> </tr></tbody></table></code></pre></figure> <p>Multiplying with a weight matrix returns an array of the desired size, and an <code class="language-plaintext highlighter-rouge">activation function</code> is used to achieve nonlinearity. The <code class="language-plaintext highlighter-rouge">backprop()</code> function is fairly straightforward: the <code class="language-plaintext highlighter-rouge">output gradient</code> is multiplied by the <code class="language-plaintext highlighter-rouge">previous input</code> to get the gradients for the weights, the <code class="language-plaintext highlighter-rouge">output gradient</code> is the bias gradient, and the <code class="language-plaintext highlighter-rouge">input gradient</code> is the product of the weights and the <code class="language-plaintext highlighter-rouge">output gradient</code>.</p> <h2 id="the-output-layer">The Output Layer</h2> <p>The <code class="language-plaintext highlighter-rouge">output layer</code> is the same as in other networks performing classification tasks: a number of features are passed in, multiplied by a weight matrix to get the correct number of outputs and run through a <code class="language-plaintext highlighter-rouge">softmax</code> function (or <code class="language-plaintext highlighter-rouge">sigmoid</code> function for binary classification) to get an output array representing the probability for each class. The only differences between the <code class="language-plaintext highlighter-rouge">output</code> and <code class="language-plaintext highlighter-rouge">feedforward layers</code> are in the <code class="language-plaintext highlighter-rouge">activation function</code> and <code class="language-plaintext highlighter-rouge">backpropagation</code>.</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr>
<td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
</pre></td> <td class="code"><pre><span class="k">class</span> <span class="nc">Softmax</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">input_len</span><span class="p">,</span> <span class="n">nodes</span><span class="p">):</span>
        <span class="n">self</span><span class="p">.</span><span class="n">weights</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="n">input_len</span><span class="p">,</span> <span class="n">nodes</span><span class="p">)</span> <span class="o">/</span> <span class="n">input_len</span>
        <span class="n">self</span><span class="p">.</span><span class="n">bias</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="n">nodes</span><span class="p">)</span> <span class="o">/</span> <span class="n">nodes</span>
    
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">):</span>
        <span class="sh">"""</span><span class="s">
        Flatten input, matrix multiply with weights, add bias, and get softmax
        </span><span class="sh">"""</span>
        <span class="c1"># Forward pass
</span>        <span class="n">totals</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">dot</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">weights</span><span class="p">)</span> <span class="o">+</span> <span class="n">self</span><span class="p">.</span><span class="n">bias</span>
        <span class="n">exp</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">exp</span><span class="p">(</span><span class="n">totals</span><span class="p">)</span>
        <span class="c1"># Saving forward pass for backpropagation
</span>        <span class="n">self</span><span class="p">.</span><span class="n">prev_input_shape</span> <span class="o">=</span> <span class="nb">input</span><span class="p">.</span><span class="n">shape</span>
        <span class="n">self</span><span class="p">.</span><span class="n">prev_input</span> <span class="o">=</span> <span class="nb">input</span>
        <span class="n">self</span><span class="p">.</span><span class="n">prev_totals</span> <span class="o">=</span> <span class="n">totals</span>
        <span class="k">return</span> <span class="n">exp</span> <span class="o">/</span> <span class="n">np</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">exp</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    
    <span class="k">def</span> <span class="nf">backprop</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">d_L_d_out</span><span class="p">,</span> <span class="n">learn_rate</span><span class="p">):</span>
        <span class="sh">"""</span><span class="s">
        Softmax backprop for output layer
        </span><span class="sh">"""</span>
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">gradient</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">(</span><span class="n">d_L_d_out</span><span class="p">):</span>
            <span class="c1"># Only the gradient at the correct class is nonzero
</span>            <span class="k">if</span> <span class="n">gradient</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                <span class="k">continue</span> 
            <span class="c1"># e^totals
</span>            <span class="n">t_exp</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">exp</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">prev_totals</span><span class="p">)</span>
            <span class="n">S</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">t_exp</span><span class="p">)</span>
            <span class="c1"># Gradients at i against totals
</span>            <span class="n">d_out_d_t</span> <span class="o">=</span> <span class="o">-</span><span class="n">t_exp</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">*</span> <span class="n">t_exp</span> <span class="o">/</span> <span class="p">(</span><span class="n">S</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span>
            <span class="n">d_out_d_t</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">t_exp</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">*</span> <span class="p">(</span><span class="n">S</span> <span class="o">-</span> <span class="n">t_exp</span><span class="p">[</span><span class="n">i</span><span class="p">])</span> <span class="o">/</span> <span class="p">(</span><span class="n">S</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span>
            <span class="c1"># Gradients of totals against weights/bias/input
</span>            <span class="n">d_t_d_w</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">prev_input</span>
            <span class="n">d_t_d_b</span> <span class="o">=</span> <span class="mi">1</span>
            <span class="n">d_t_d_inputs</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">weights</span>
            <span class="c1"># Gradients of loss against totals
</span>            <span class="n">d_L_d_t</span> <span class="o">=</span> <span class="n">gradient</span> <span class="o">*</span> <span class="n">d_out_d_t</span>
            <span class="n">d_L_d_w</span> <span class="o">=</span> <span class="n">d_t_d_w</span><span class="p">[</span><span class="n">np</span><span class="p">.</span><span class="n">newaxis</span><span class="p">].</span><span class="n">T</span> <span class="o">@</span> <span class="n">d_L_d_t</span><span class="p">[</span><span class="n">np</span><span class="p">.</span><span class="n">newaxis</span><span class="p">]</span>
            <span class="n">d_L_d_b</span> <span class="o">=</span> <span class="n">d_L_d_t</span> <span class="o">*</span> <span class="n">d_t_d_b</span>
            <span class="n">d_L_d_inputs</span> <span class="o">=</span> <span class="n">d_t_d_inputs</span> <span class="o">@</span> <span class="n">d_L_d_t</span>
            <span class="c1"># Update weights and bias
</span>            <span class="n">self</span><span class="p">.</span><span class="n">weights</span> <span class="o">-=</span> <span class="n">learn_rate</span> <span class="o">*</span> <span class="n">d_L_d_w</span>
            <span class="n">self</span><span class="p">.</span><span class="n">bias</span> <span class="o">-=</span> <span class="n">learn_rate</span> <span class="o">*</span> <span class="n">d_L_d_b</span>
            <span class="k">return</span> <span class="n">d_L_d_inputs</span><span class="p">.</span><span class="nf">reshape</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">prev_input_shape</span><span class="p">)</span>
</pre></td> </tr></tbody></table></code></pre></figure> <p>Generally, the <code class="language-plaintext highlighter-rouge">output layer</code> always uses a <code class="language-plaintext highlighter-rouge">sigmoid</code> or <code class="language-plaintext highlighter-rouge">softmax</code> activation function since these functions convert unbounded numbers into probabilities. However, since these functions are prone to vanishing gradient issues, they are almost never used for intermediary layers. For convenience, I implement the <code class="language-plaintext highlighter-rouge">softmax</code> function directly into the <code class="language-plaintext highlighter-rouge">output layer</code> class, although they can be separated as with the <code class="language-plaintext highlighter-rouge">ReLU</code> function. The <code class="language-plaintext highlighter-rouge">backprop()</code> function for the <code class="language-plaintext highlighter-rouge">output layer</code> needs to take into account the fact that the gradient is only nonzero for the gold standard class but is otherwise the same as in the <code class="language-plaintext highlighter-rouge">feedforward layer</code>.</p> <h2 id="model-architecture">Model Architecture</h2> <p>All the layers are done, let’s put them together!</p> <div class="row justify-content-md-center"> <div class="col-sm-auto mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/projects/cnn_architecture-480.webp 480w,/assets/img/projects/cnn_architecture-800.webp 800w,/assets/img/projects/cnn_architecture-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/projects/cnn_architecture.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="simple cnn architecture" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p>As you can see, this is the simplest architecture that uses all of the layers of the CNN. At the same time, we can add as many <code class="language-plaintext highlighter-rouge">convolutional layer -&gt; convolutional layer -&gt; pooling layer</code> stacks as we want for arbitrary depth. Depending on the size of the model, we can also have multiple feedforward layers to increase the predictive power of the classification portion of the network. Empirically, lower convolutional layers in deep networks learn to identify basic shapes such as edges whereas higher layers learn more complex features such as body parts.</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr>
<td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
59
60
61
62
63
64
65
66
67
68
69
70
71
72
73
74
75
76
77
78
79
80
81
82
83
84
85
86
87
88
89
90
91
92
93
94
95
96
97
98
99
100
101
102
103
104
105
106
107
108
109
110
111
112
113
114
115
116
117
118
119
120
121
122
123
124
125
126
127
128
129
130
131
132
133
134
</pre></td> <td class="code"><pre><span class="k">class</span> <span class="nc">SimpleCNN</span><span class="p">:</span>
    <span class="sh">"""</span><span class="s">
    Simple CNN using the layers built above.
    Structure:
    Input -&gt; Conv -&gt; ReLU -&gt; Conv -&gt; ReLU -&gt; MaxPool -&gt; FeedForward -&gt; ReLU -&gt; Softmax
    </span><span class="sh">"""</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">ConvLayer_1</span><span class="p">,</span> <span class="n">ReLU_1</span><span class="p">,</span> <span class="n">ConvLayer_2</span><span class="p">,</span> <span class="n">ReLU_2</span><span class="p">,</span> <span class="n">MaxPool</span><span class="p">,</span> <span class="n">FeedForward</span><span class="p">,</span> <span class="n">ReLU_3</span><span class="p">,</span> <span class="n">Output</span><span class="p">):</span>
        <span class="n">self</span><span class="p">.</span><span class="n">ConvLayer_1</span> <span class="o">=</span> <span class="n">ConvLayer_1</span>
        <span class="n">self</span><span class="p">.</span><span class="n">ReLU_1</span> <span class="o">=</span> <span class="n">ReLU_1</span>
        <span class="n">self</span><span class="p">.</span><span class="n">ConvLayer_2</span> <span class="o">=</span> <span class="n">ConvLayer_2</span>
        <span class="n">self</span><span class="p">.</span><span class="n">ReLU_2</span> <span class="o">=</span> <span class="n">ReLU_2</span>
        <span class="n">self</span><span class="p">.</span><span class="n">MaxPool</span> <span class="o">=</span> <span class="n">MaxPool</span>
        <span class="n">self</span><span class="p">.</span><span class="n">FeedForward</span> <span class="o">=</span> <span class="n">FeedForward</span>
        <span class="n">self</span><span class="p">.</span><span class="n">ReLU_3</span> <span class="o">=</span> <span class="n">ReLU_3</span>
        <span class="n">self</span><span class="p">.</span><span class="n">OutputLayer</span> <span class="o">=</span> <span class="n">Output</span>
    
    <span class="k">def</span> <span class="nf">preprocess</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">data</span><span class="p">):</span>
        <span class="sh">"""</span><span class="s">
        Data generally needs to be reshaped for our purposes
        </span><span class="sh">"""</span>
        <span class="k">if</span> <span class="nf">len</span><span class="p">(</span><span class="n">data</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">==</span> <span class="mi">3</span><span class="p">:</span>
            <span class="n">data</span> <span class="o">=</span> <span class="n">data</span><span class="p">[:,</span> <span class="n">np</span><span class="p">.</span><span class="n">newaxis</span><span class="p">,</span> <span class="p">:,</span> <span class="p">:]</span>
        <span class="k">elif</span> <span class="nf">len</span><span class="p">(</span><span class="n">data</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">==</span> <span class="mi">4</span> <span class="ow">and</span> <span class="n">data</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span> <span class="o">==</span> <span class="mi">3</span><span class="p">:</span>
            <span class="n">data</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">moveaxis</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">data</span>
    
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">image</span><span class="p">):</span>
        <span class="sh">"""</span><span class="s">
        Forward pass through network, transform image from [0, 255] to [-0.5, 0.5] as standard practice
        </span><span class="sh">"""</span>
        <span class="nb">input</span> <span class="o">=</span> <span class="p">(</span><span class="n">image</span> <span class="o">/</span> <span class="mi">255</span><span class="p">)</span> <span class="o">-</span> <span class="mf">0.5</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">ConvLayer_1</span><span class="p">.</span><span class="nf">forward</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">ReLU_1</span><span class="p">.</span><span class="nf">forward</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">ConvLayer_2</span><span class="p">.</span><span class="nf">forward</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">ReLU_2</span><span class="p">.</span><span class="nf">forward</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">MaxPool</span><span class="p">.</span><span class="nf">forward</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">FeedForward</span><span class="p">.</span><span class="nf">forward</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">ReLU_3</span><span class="p">.</span><span class="nf">forward</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">OutputLayer</span><span class="p">.</span><span class="nf">forward</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">out</span>
    
    <span class="k">def</span> <span class="nf">backprop</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">gradient</span><span class="p">,</span> <span class="n">learn_rate</span><span class="p">):</span>
        <span class="sh">"""</span><span class="s">
        Backpropagation through network
        </span><span class="sh">"""</span>
        <span class="n">d_L_d_out</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">OutputLayer</span><span class="p">.</span><span class="nf">backprop</span><span class="p">(</span><span class="n">gradient</span><span class="p">,</span> <span class="n">learn_rate</span><span class="p">)</span>
        <span class="n">d_L_d_out</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">ReLU_3</span><span class="p">.</span><span class="nf">backprop</span><span class="p">(</span><span class="n">d_L_d_out</span><span class="p">)</span>
        <span class="n">d_L_d_out</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">FeedForward</span><span class="p">.</span><span class="nf">backprop</span><span class="p">(</span><span class="n">d_L_d_out</span><span class="p">,</span> <span class="n">learn_rate</span><span class="p">)</span>
        <span class="n">d_L_d_out</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">MaxPool</span><span class="p">.</span><span class="nf">backprop</span><span class="p">(</span><span class="n">d_L_d_out</span><span class="p">)</span>
        <span class="n">d_L_d_out</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">ReLU_2</span><span class="p">.</span><span class="nf">backprop</span><span class="p">(</span><span class="n">d_L_d_out</span><span class="p">)</span>
        <span class="n">d_L_d_out</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">ConvLayer_2</span><span class="p">.</span><span class="nf">backprop</span><span class="p">(</span><span class="n">d_L_d_out</span><span class="p">,</span> <span class="n">learn_rate</span><span class="p">)</span>
        <span class="n">d_L_d_out</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">ReLU_1</span><span class="p">.</span><span class="nf">backprop</span><span class="p">(</span><span class="n">d_L_d_out</span><span class="p">)</span>
        <span class="n">d_L_d_out</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">ConvLayer_1</span><span class="p">.</span><span class="nf">backprop</span><span class="p">(</span><span class="n">d_L_d_out</span><span class="p">,</span> <span class="n">learn_rate</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">d_L_d_out</span>

    <span class="k">def</span> <span class="nf">avg_f1_score</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">predicted_labels</span><span class="p">,</span> <span class="n">true_labels</span><span class="p">,</span> <span class="n">classes</span><span class="p">):</span>
        <span class="sh">"""</span><span class="s">
        Calculate the f1-score for each class and return the average of it
        F1 score is the harmonic mean of precision and recall
        Precision is True Positives / All Positives Predictions
        Recall is True Positives / All Positive Labelsß
        </span><span class="sh">"""</span>
        <span class="n">f1_scores</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">c</span> <span class="ow">in</span> <span class="n">classes</span><span class="p">:</span>
            <span class="n">pred_class</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([</span><span class="n">pred</span> <span class="o">==</span> <span class="n">c</span> <span class="k">for</span> <span class="n">pred</span> <span class="ow">in</span> <span class="n">predicted_labels</span><span class="p">])</span>
            <span class="n">true_class</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([</span><span class="n">lab</span> <span class="o">==</span> <span class="n">c</span> <span class="k">for</span> <span class="n">lab</span> <span class="ow">in</span> <span class="n">true_labels</span><span class="p">])</span>
            <span class="n">precision</span> <span class="o">=</span> <span class="p">(</span><span class="nf">t_sum</span><span class="p">(</span><span class="nf">logical_and</span><span class="p">(</span><span class="n">pred_class</span><span class="p">,</span> <span class="n">true_class</span><span class="p">))</span> <span class="o">/</span> <span class="nf">t_sum</span><span class="p">(</span><span class="n">pred_class</span><span class="p">))</span> <span class="k">if</span> <span class="nf">t_sum</span><span class="p">(</span><span class="n">pred_class</span><span class="p">)</span> <span class="k">else</span> <span class="mi">0</span>
            <span class="n">recall</span> <span class="o">=</span> <span class="nf">t_sum</span><span class="p">(</span><span class="nf">logical_and</span><span class="p">(</span><span class="n">pred_class</span><span class="p">,</span> <span class="n">true_class</span><span class="p">))</span> <span class="o">/</span> <span class="nf">t_sum</span><span class="p">(</span><span class="n">true_class</span><span class="p">)</span><span class="k">if</span> <span class="nf">t_sum</span><span class="p">(</span><span class="n">true_class</span><span class="p">)</span> <span class="k">else</span> <span class="mi">0</span>
            <span class="n">f1_scores</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="p">(</span><span class="n">precision</span> <span class="o">*</span> <span class="n">recall</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">precision</span> <span class="o">+</span> <span class="n">recall</span><span class="p">))</span> <span class="k">if</span> <span class="n">precision</span> <span class="ow">and</span> <span class="n">recall</span> <span class="k">else</span> <span class="mi">0</span>
        <span class="k">return</span> <span class="n">np</span><span class="p">.</span><span class="nf">mean</span><span class="p">(</span><span class="n">f1_scores</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">dataset</span><span class="p">,</span> <span class="n">true_labels</span><span class="p">,</span> <span class="n">classes</span><span class="p">):</span>
        <span class="sh">"""</span><span class="s">
        Predict labels for dataset and return f1-score
        </span><span class="sh">"""</span>
        <span class="n">preds</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">acc</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">for</span> <span class="n">im</span><span class="p">,</span> <span class="n">lab</span> <span class="ow">in</span> <span class="nf">zip</span><span class="p">(</span><span class="n">dataset</span><span class="p">,</span> <span class="n">true_labels</span><span class="p">):</span>
            <span class="n">preds</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">argmax</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="nf">forward</span><span class="p">(</span><span class="n">im</span><span class="p">)))</span>
            <span class="n">acc</span> <span class="o">+=</span> <span class="p">(</span><span class="n">preds</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">==</span> <span class="n">lab</span><span class="p">)</span>
        <span class="n">preds</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span><span class="n">preds</span><span class="p">)</span>
        <span class="n">accuracy</span> <span class="o">=</span> <span class="n">acc</span> <span class="o">/</span> <span class="nf">len</span><span class="p">(</span><span class="n">preds</span><span class="p">)</span>
        <span class="n">f1</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">avg_f1_score</span><span class="p">(</span><span class="n">preds</span><span class="p">,</span> <span class="n">true_labels</span><span class="p">,</span> <span class="n">classes</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">accuracy</span><span class="p">,</span> <span class="n">f1</span>

    <span class="k">def</span> <span class="nf">train</span><span class="p">(</span>
        <span class="n">self</span><span class="p">,</span>
        <span class="n">trainset</span><span class="p">,</span>
        <span class="n">trainlabels</span><span class="p">,</span>
        <span class="n">devset</span><span class="p">,</span>
        <span class="n">devlabels</span><span class="p">,</span>
        <span class="n">classes</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">9</span><span class="p">],</span>
        <span class="n">epochs</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>
        <span class="n">learn_rate</span><span class="o">=</span><span class="mf">0.005</span>
    <span class="p">):</span>
        <span class="sh">"""</span><span class="s">
        Training loop for network
        </span><span class="sh">"""</span>
        <span class="c1"># Preprocess &amp; generate permutation to shuffle data
</span>        <span class="n">trainset</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">preprocess</span><span class="p">(</span><span class="n">trainset</span><span class="p">)</span>
        <span class="n">devset</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">preprocess</span><span class="p">(</span><span class="n">devset</span><span class="p">)</span>
        <span class="n">permutation</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">permutation</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">trainset</span><span class="p">))</span>
        <span class="n">train_data</span> <span class="o">=</span> <span class="n">trainset</span><span class="p">[</span><span class="n">permutation</span><span class="p">]</span>
        <span class="n">train_labels</span> <span class="o">=</span> <span class="n">trainlabels</span><span class="p">[</span><span class="n">permutation</span><span class="p">]</span>
        <span class="c1"># Training loop
</span>        <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Training...</span><span class="sh">"</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">epochs</span><span class="p">):</span>
            <span class="n">losses</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="k">for</span> <span class="n">image</span><span class="p">,</span> <span class="n">label</span> <span class="ow">in</span> <span class="nf">tqdm</span><span class="p">(</span><span class="nf">list</span><span class="p">(</span><span class="nf">zip</span><span class="p">(</span><span class="n">train_data</span><span class="p">,</span> <span class="n">train_labels</span><span class="p">))):</span>
                <span class="c1"># Forward pass
</span>                <span class="n">out</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">forward</span><span class="p">(</span><span class="n">image</span><span class="p">)</span>
                <span class="c1"># Calculate loss and gradient
</span>                <span class="n">loss</span> <span class="o">=</span> <span class="o">-</span><span class="n">np</span><span class="p">.</span><span class="nf">log</span><span class="p">(</span><span class="n">out</span><span class="p">[</span><span class="n">label</span><span class="p">])</span>
                <span class="n">losses</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>
                <span class="n">gradient</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">zeros</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span>
                <span class="n">gradient</span><span class="p">[</span><span class="n">label</span><span class="p">]</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span> <span class="o">/</span> <span class="n">out</span><span class="p">[</span><span class="n">label</span><span class="p">]</span>
                <span class="c1"># Backpropagation
</span>                <span class="n">self</span><span class="p">.</span><span class="nf">backprop</span><span class="p">(</span><span class="n">gradient</span><span class="p">,</span> <span class="n">learn_rate</span><span class="p">)</span>
            <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Epoch </span><span class="si">{</span><span class="n">epoch</span> <span class="o">+</span> <span class="mi">1</span><span class="si">}</span><span class="s">, loss: </span><span class="si">{</span><span class="n">np</span><span class="p">.</span><span class="nf">mean</span><span class="p">(</span><span class="n">losses</span><span class="p">)</span><span class="si">:</span><span class="p">.</span><span class="mi">3</span><span class="n">f</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
            <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Evaluating dev...</span><span class="sh">"</span><span class="p">)</span>
            <span class="n">acc</span><span class="p">,</span> <span class="n">f1</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">predict</span><span class="p">(</span><span class="n">devset</span><span class="p">,</span> <span class="n">devlabels</span><span class="p">,</span> <span class="n">classes</span><span class="p">)</span>
            <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Dev Accuracy: </span><span class="si">{</span><span class="n">acc</span><span class="si">:</span><span class="p">.</span><span class="mi">3</span><span class="n">f</span><span class="si">}</span><span class="s">, Dev F1 Score: </span><span class="si">{</span><span class="n">f1</span><span class="si">:</span><span class="p">.</span><span class="mi">3</span><span class="n">f</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>

<span class="n">model</span> <span class="o">=</span> <span class="nc">SimpleCNN</span><span class="p">(</span>
        <span class="nc">ConvLayer</span><span class="p">(</span><span class="n">input_shape</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">),</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">num_kernels</span><span class="o">=</span><span class="mi">6</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">0</span><span class="p">),</span>
        <span class="nc">ReLU</span><span class="p">(),</span>
        <span class="nc">ConvLayer</span><span class="p">(</span><span class="n">input_shape</span><span class="o">=</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">),</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">num_kernels</span><span class="o">=</span><span class="mi">6</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">0</span><span class="p">),</span>
        <span class="nc">ReLU</span><span class="p">(),</span>
        <span class="nc">MaxPool</span><span class="p">(),</span>
        <span class="nc">FeedForward</span><span class="p">(</span><span class="mi">6</span> <span class="o">*</span> <span class="mi">14</span> <span class="o">*</span> <span class="mi">14</span><span class="p">,</span> <span class="mi">100</span><span class="p">),</span>
        <span class="nc">ReLU</span><span class="p">(),</span>
        <span class="nc">Softmax</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
<span class="p">)</span>
<span class="n">model</span><span class="p">.</span><span class="nf">train</span><span class="p">(</span><span class="n">trainset</span><span class="p">,</span> <span class="n">trainlabels</span><span class="p">,</span> <span class="n">testset</span><span class="p">,</span> <span class="n">testlabels</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">learn_rate</span><span class="o">=</span><span class="mf">0.005</span><span class="p">)</span>
</pre></td> </tr></tbody></table></code></pre></figure> <p>The model class and training loop are to implement since the <code class="language-plaintext highlighter-rouge">forward()</code> and <code class="language-plaintext highlighter-rouge">backprop()</code> functions have been properly implemented. Before training, I use <code class="language-plaintext highlighter-rouge">np.random.permutation()</code> to randomize the training set in case the original initialization is suboptimal. Although the dataset is in black and white, the layers are built to handle 3D inputs like RGB images. As a result, a preprocess function is needed to handle the 2D inputs in the Fashion MNIST dataset and to adjust the channel dimension for RGB images. RGB images are usually processed as <code class="language-plaintext highlighter-rouge">height x width x channel</code> matrices, however, for convenient matrix multiplication the channel matrix needs to be moved up to <code class="language-plaintext highlighter-rouge">channel x height x width</code>. Using this implementation, the multiplication done in the <code class="language-plaintext highlighter-rouge">convolutional layer</code> can be done natively in <code class="language-plaintext highlighter-rouge">NumPy</code> instead of utilizing additional for loops.</p> <h2 id="benchmarking-with-keras">Benchmarking with Keras</h2> <figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr>
<td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
</pre></td> <td class="code"><pre><span class="kn">from</span> <span class="n">keras.models</span> <span class="kn">import</span> <span class="n">Sequential</span>
<span class="kn">from</span> <span class="n">keras.layers</span> <span class="kn">import</span> <span class="n">Conv2D</span><span class="p">,</span> <span class="n">MaxPooling2D</span><span class="p">,</span> <span class="n">Dense</span><span class="p">,</span> <span class="n">Flatten</span>
<span class="kn">from</span> <span class="n">keras.utils</span> <span class="kn">import</span> <span class="n">to_categorical</span>
<span class="kn">from</span> <span class="n">keras.optimizers.legacy</span> <span class="kn">import</span> <span class="n">SGD</span>

<span class="n">train_images</span> <span class="o">=</span> <span class="p">(</span><span class="n">trainset</span> <span class="o">/</span> <span class="mi">255</span><span class="p">)</span> <span class="o">-</span> <span class="mf">0.5</span>
<span class="n">test_images</span> <span class="o">=</span> <span class="p">(</span><span class="n">testset</span> <span class="o">/</span> <span class="mi">255</span><span class="p">)</span> <span class="o">-</span> <span class="mf">0.5</span>

<span class="n">train_images</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">expand_dims</span><span class="p">(</span><span class="n">train_images</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<span class="n">test_images</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">expand_dims</span><span class="p">(</span><span class="n">test_images</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>

<span class="n">model</span> <span class="o">=</span> <span class="nc">Sequential</span><span class="p">([</span>
  <span class="nc">Conv2D</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="sh">"</span><span class="s">same</span><span class="sh">"</span><span class="p">,</span> <span class="n">input_shape</span><span class="o">=</span><span class="p">(</span><span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">use_bias</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="sh">'</span><span class="s">relu</span><span class="sh">'</span><span class="p">),</span>
  <span class="nc">Conv2D</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="sh">"</span><span class="s">same</span><span class="sh">"</span><span class="p">,</span> <span class="n">input_shape</span><span class="o">=</span><span class="p">(</span><span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">,</span> <span class="mi">6</span><span class="p">),</span> <span class="n">use_bias</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="sh">'</span><span class="s">relu</span><span class="sh">'</span><span class="p">),</span>
  <span class="nc">MaxPooling2D</span><span class="p">(</span><span class="n">pool_size</span><span class="o">=</span><span class="mi">2</span><span class="p">),</span>
  <span class="nc">Flatten</span><span class="p">(),</span>
  <span class="nc">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="sh">'</span><span class="s">relu</span><span class="sh">'</span><span class="p">),</span>
  <span class="nc">Dense</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="sh">'</span><span class="s">softmax</span><span class="sh">'</span><span class="p">),</span>
<span class="p">])</span>

<span class="n">model</span><span class="p">.</span><span class="nf">compile</span><span class="p">(</span><span class="nc">SGD</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="p">.</span><span class="mi">005</span><span class="p">),</span> <span class="n">loss</span><span class="o">=</span><span class="sh">'</span><span class="s">categorical_crossentropy</span><span class="sh">'</span><span class="p">,</span> <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="sh">'</span><span class="s">accuracy</span><span class="sh">'</span><span class="p">])</span>

<span class="n">model</span><span class="p">.</span><span class="nf">fit</span><span class="p">(</span>
  <span class="n">train_images</span><span class="p">,</span>
  <span class="nf">to_categorical</span><span class="p">(</span><span class="n">trainlabels</span><span class="p">),</span>
  <span class="n">batch_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
  <span class="n">epochs</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>
  <span class="n">validation_data</span><span class="o">=</span><span class="p">(</span><span class="n">test_images</span><span class="p">,</span> <span class="nf">to_categorical</span><span class="p">(</span><span class="n">testlabels</span><span class="p">)),</span>
<span class="p">)</span>
</pre></td> </tr></tbody></table></code></pre></figure> <p>Here’s a <code class="language-plaintext highlighter-rouge">Keras</code> model implemented using the exact same architecture as above to benchmark performance. Since <code class="language-plaintext highlighter-rouge">Keras</code> is an optimized library with widespread adoption, running time is predictably orders of magnitude faster than my implementation.</p> <h5 id="results">Results</h5> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>NumPy model: F1 = 0.890, Accuracy = 0.891
Keras model: Accuracy = 0.887
</code></pre></div></div> <p>My model surprisingly outperforms the <code class="language-plaintext highlighter-rouge">Keras</code> model by a little, which suggests that the underlying mechanisms in both implementations are the same. I achieve an <code class="language-plaintext highlighter-rouge">F1 = 0.890</code> training <code class="language-plaintext highlighter-rouge">3</code> epochs on the entire dataset, taking roughly half an hour per epoch. While these results are significantly lower than the state of the art <code class="language-plaintext highlighter-rouge">F1 = 0.97</code>, theoretically a deeper network using the same layers could potentially achieve comparable results with enough computation.</p> <h3 id="sources">Sources</h3> <ul> <li> <a href="https://pytorch.org/vision/stable/generated/torchvision.datasets.FashionMNIST.html#torchvision.datasets.FashionMNIST" rel="external nofollow noopener" target="_blank">Fashion MNIST Dataset</a> (<a href="https://github.com/zalandoresearch/fashion-mnist" rel="external nofollow noopener" target="_blank">GitHub Repo</a>)</li> <li><a href="https://victorzhou.com/blog/intro-to-cnns-part-1/" rel="external nofollow noopener" target="_blank">In-Depth Tutorial (Part 1)</a></li> <li><a href="https://victorzhou.com/blog/intro-to-cnns-part-2/" rel="external nofollow noopener" target="_blank">In-Depth Tutorial (Part 2)</a></li> <li><a href="https://www.youtube.com/watch?v=Lakz2MoHy6o" rel="external nofollow noopener" target="_blank">In-Depth YouTube Tutorial</a></li> <li><a href="https://www.geeksforgeeks.org/introduction-convolution-neural-network/" rel="external nofollow noopener" target="_blank">High Level Introduction</a></li> <li><a href="https://towardsdatascience.com/a-guide-to-convolutional-neural-networks-from-scratch-f1e3bfc3e2de" rel="external nofollow noopener" target="_blank">Understanding Each Layer</a></li> <li><a href="https://towardsdatascience.com/convolution-vs-cross-correlation-81ec4a0ec253" rel="external nofollow noopener" target="_blank">Convolution vs. Cross-Correlation</a></li> <li><a href="https://towardsdatascience.com/coding-neural-network-forward-propagation-and-backpropagtion-ccf8cf369f76" rel="external nofollow noopener" target="_blank">Activation Function Backpropagation</a></li> </ul> </article> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2024 Alvin Chen. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Last updated: June 10, 2024. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2930004b8d7fcd0a8e00fdcfc8fc9f24"></script> <script defer src="/assets/js/common.js?4a129fbf39254905f505c7246e641eaf"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>