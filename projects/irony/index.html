<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> irony detector | Alvin Chen 陳柏駿 </title> <meta name="author" content="Alvin Chen 陳柏駿"> <meta name="description" content="A BiLSTM that detects irony in tweets based off SemEval 2018 Task 3."> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/favicon.ico?c6fddee0e29e79c86a7dd97fa38f204b"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://alvin-pc-chen.github.io/projects/irony/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Alvin</span> Chen 陳柏駿 </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item active"> <a class="nav-link" href="/projects/">projects <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">repositories </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">irony detector</h1> <p class="post-description">A BiLSTM that detects irony in tweets based off SemEval 2018 Task 3.</p> </header> <article> <div class="row justify-content-md-center"> <div class="col-sm-auto mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/projects/irony_title_card-480.webp 480w,/assets/img/projects/irony_title_card-800.webp 800w,/assets/img/projects/irony_title_card-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/projects/irony_title_card.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="title card" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p>In this project, I implemented an Irony Detection model that performs slightly better than the 2018 SOTA. This project was started as part of the NLP course at CU Boulder, although I’ve made a few adjustments.</p> <p>The model uses <strong><a href="https://nlp.stanford.edu/projects/glove/" rel="external nofollow noopener" target="_blank">GloVe embeddings</a></strong> to encode the input tweets before feeding them to a <code class="language-plaintext highlighter-rouge">PyTorch</code> module comprising of several <code class="language-plaintext highlighter-rouge">BiLSTM</code> layers, a <code class="language-plaintext highlighter-rouge">feedforward layer</code>, and a <code class="language-plaintext highlighter-rouge">softmax</code> layer to produce the classification result. I trained the model using a variety of hyperperameters in order to get the ideal results, achieving an <code class="language-plaintext highlighter-rouge">F1 = 0.713</code> which was higher than the <strong><a href="https://competitions.codalab.org/competitions/17468#results" rel="external nofollow noopener" target="_blank">top result</a></strong> on the SemEval subtask (see Evaluation Task A).</p> <p>Code for this project can be found <strong><a href="https://github.com/alvin-pc-chen/semeval_irony_detector" rel="external nofollow noopener" target="_blank">here</a></strong>. While most of the code is shown in this deep dive, many utility functions are left out for brevity.</p> <h2 id="table-of-contents">Table of Contents</h2> <ol> <li><a href="#irony-detection">Irony Detection</a></li> <li><a href="#tokenization">Tokenization</a></li> <li><a href="#glove-embeddings">GloVe Embeddings</a></li> <li><a href="#batching-and-encoding">Batching and Encoding</a></li> <li><a href="#modeling">Modeling</a></li> <li><a href="#training">Training</a></li> <li><a href="#results">Results</a></li> <li><a href="#sources">Sources</a></li> </ol> <h2 id="irony-detection">Irony Detection</h2> <p>The <strong><a href="https://github.com/Cyvhee/SemEval2018-Task3" rel="external nofollow noopener" target="_blank">SemEval 2018 Task 3</a></strong> was aimed at developing a working irony detector from annotated tweets, which is an interesting task from both a linguistic and an application perspective. Linguistically, irony is a nebulous concept that relies on a high level semantic understanding of the text; a system that can detect irony helps us understand the underlying patterns of this phenomenon. In terms of application, the ability to detect irony is useful for downstream tasks such as logical inference. However, for the same reasons that makes this an interesting task, irony detection faces two nontrivial challenges:</p> <ol> <li>Ambiguity of irony in natural language hindering easy feature extraction;</li> <li>The small dataset of only 3,834 tweets, preventing end-to-end training of a larger language model.</li> </ol> <p>In order to solve this task, our language model needs to have an understanding of natural language before training on the limited labeled data that we have. Enter <strong><a href="https://nlp.stanford.edu/projects/glove/" rel="external nofollow noopener" target="_blank">Global Vectors for Word Representation</a></strong> (GloVe), distributional semantic embeddings learned from a large corpus. By encoding tweets using these embeddings, we essentially initialize our model with some amount of natural language understanding. The model weights can instead focus on learning irony instead of word meaning, which dramatically <strong><a href="/projects/cnn/#why-convolutional-neural-networks">reduces the search space</a></strong> for the task.</p> <p>The process is as follows:</p> <ol> <li>Tokenize the datset;</li> <li>Initialize <code class="language-plaintext highlighter-rouge">GloVe</code> embeddings using token vocabulary;</li> <li>Create index to encode tokens;</li> <li>Initialize <code class="language-plaintext highlighter-rouge">BiLSTM</code> model using <code class="language-plaintext highlighter-rouge">PyTorch</code>;</li> <li>Train and test model.</li> </ol> <div class="row justify-content-md-center"> <div class="col-sm-auto mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/projects/irony_bilstm-480.webp 480w,/assets/img/projects/irony_bilstm-800.webp 800w,/assets/img/projects/irony_bilstm-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/projects/irony_bilstm.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="bilstm structure" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p>Since we use pretrained embeddings and have gold standard labels for supervised learning, we can use the powerful <strong><a href="https://paperswithcode.com/method/bilstm" rel="external nofollow noopener" target="_blank">BiLSTM</a></strong> model. The <strong><a href="https://colah.github.io/posts/2015-08-Understanding-LSTMs/" rel="external nofollow noopener" target="_blank">Long Short-Term Memory</a></strong> model is a neural network developed on the assumption that earlier words in a text provide contextual information for later words. It improves on the <strong><a href="https://www.geeksforgeeks.org/introduction-to-recurrent-neural-network/" rel="external nofollow noopener" target="_blank">Recurrent Neural Network</a></strong> (RNN) by adding a <code class="language-plaintext highlighter-rouge">forget</code> gate, a set of learned weights that remove irrelevant information between iterations.</p> <p>The Bidirectional Long Short-Term Memory model makes the further assumption that later words in the text also provide important information for understanding earlier words. These assumptions are implemented by stacking two <code class="language-plaintext highlighter-rouge">LSTM</code> layers on top of each other going in opposite directions. The second layer takes the input text as well as the output from the first <code class="language-plaintext highlighter-rouge">LSTM</code> as input which allows it to capture the contextual information going in both directions.</p> <p>The weakness of using a bidirectional network for language modeling is in training. Single direction <code class="language-plaintext highlighter-rouge">LSTMs</code> or <code class="language-plaintext highlighter-rouge">RNNs</code> do not know what words come after the current word being processed in the input, which allows for <strong><a href="https://www.geeksforgeeks.org/self-supervised-learning-ssl/" rel="external nofollow noopener" target="_blank">self-supervised</a></strong> learning by training the model on predicting the next word. Bidirectional models, on the other hand, need to see the full sentence front and back in order to capture all of the contextual information, which prevents this type of training. Although self-supervised learning is incredibly useful when labeled data is scarce, since we have both <code class="language-plaintext highlighter-rouge">GloVe</code> embeddings to capture semantic information and some labeled training data, we can train a <code class="language-plaintext highlighter-rouge">BiLSTM</code> model for this classification task.</p> <h2 id="tokenization">Tokenization</h2> <p>Before using <code class="language-plaintext highlighter-rouge">GloVe</code> embeddings we first need to process them into uniform atoms that can be encoded as embeddings. As you can imagine, there’s a great diversity in the way tweets are written: typos, emojis, or different spellings for exaggeration are all common. We create uniformity by breaking tweets down into <code class="language-plaintext highlighter-rouge">tokens</code>: most commonly occurring word, subword, emoji, or other part of text. How this is done depends on the <code class="language-plaintext highlighter-rouge">tokenizer</code>, a basic building block of language models. For this project, we use a <code class="language-plaintext highlighter-rouge">tokenizer</code> built in <code class="language-plaintext highlighter-rouge">spaCy</code> which has a high overlap with the tokens in <code class="language-plaintext highlighter-rouge">GloVe</code>:</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr>
<td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
</pre></td> <td class="code"><pre><span class="kn">from</span> <span class="n">typing</span> <span class="kn">import</span> <span class="n">List</span><span class="p">,</span> <span class="n">Optional</span>
<span class="kn">import</span> <span class="n">spacy</span>

<span class="k">class</span> <span class="nc">Tokenizer</span><span class="p">:</span>
    <span class="sh">"""</span><span class="s">Tokenizes and pads a batch of input sentences.</span><span class="sh">"""</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">pad_symbol</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="sh">"</span><span class="s">&lt;PAD&gt;</span><span class="sh">"</span><span class="p">):</span>
        <span class="sh">"""</span><span class="s">Initializes the tokenizer.
        Args:
            pad_symbol (Optional[str], optional): The symbol for a pad. Defaults to </span><span class="sh">"</span><span class="s">&lt;PAD&gt;</span><span class="sh">"</span><span class="s">.
        </span><span class="sh">"""</span>
        <span class="n">self</span><span class="p">.</span><span class="n">pad_symbol</span> <span class="o">=</span> <span class="n">pad_symbol</span>
        <span class="n">self</span><span class="p">.</span><span class="n">nlp</span> <span class="o">=</span> <span class="n">spacy</span><span class="p">.</span><span class="nf">load</span><span class="p">(</span><span class="sh">"</span><span class="s">en_core_web_sm</span><span class="sh">"</span><span class="p">)</span>
    
    <span class="k">def</span> <span class="nf">__call__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]]:</span>
        <span class="sh">"""</span><span class="s">Tokenizes each sentence in the batch, and pads them if necessary so
        that we have equal length sentences in the batch.
        Args:
            batch (List[str]): A List of sentence strings.
        Returns:
            List[List[str]]: A List of equal-length token Lists.
        </span><span class="sh">"""</span>
        <span class="n">batch</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">tokenize</span><span class="p">(</span><span class="n">batch</span><span class="p">)</span>
        <span class="n">batch</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">pad</span><span class="p">(</span><span class="n">batch</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">batch</span>

    <span class="k">def</span> <span class="nf">tokenize</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">sentences</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]]:</span>
        <span class="sh">"""</span><span class="s">Tokenizes the List of string sentences into a Lists of tokens using spacy tokenizer.
        Args:
            sentences (List[str]): The input sentence.
        Returns:
            List[str]: The tokenized version of the sentence.
        </span><span class="sh">"""</span>
        <span class="n">tokens</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">sentence</span> <span class="ow">in</span> <span class="n">sentences</span><span class="p">:</span>
            <span class="n">t</span> <span class="o">=</span> <span class="p">[</span><span class="sh">"</span><span class="s">&lt;SOS&gt;</span><span class="sh">"</span><span class="p">]</span>
            <span class="k">for</span> <span class="n">token</span> <span class="ow">in</span> <span class="n">self</span><span class="p">.</span><span class="nf">nlp</span><span class="p">(</span><span class="n">sentence</span><span class="p">):</span>
                <span class="n">t</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">token</span><span class="p">.</span><span class="n">text</span><span class="p">)</span>
            <span class="n">t</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="sh">"</span><span class="s">&lt;EOS&gt;</span><span class="sh">"</span><span class="p">)</span>
            <span class="n">tokens</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">t</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">tokens</span>

    <span class="k">def</span> <span class="nf">pad</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]])</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]]:</span>
        <span class="sh">"""</span><span class="s">Appends pad symbols to each tokenized sentence in the batch such that
        every List of tokens is the same length. This means that the max length sentence
        will not be padded.
        Args:
            batch (List[List[str]]): Batch of tokenized sentences.
        Returns:
            List[List[str]]: Batch of padded tokenized sentences. 
        </span><span class="sh">"""</span>
        <span class="n">maxlen</span> <span class="o">=</span> <span class="nf">max</span><span class="p">([</span><span class="nf">len</span><span class="p">(</span><span class="n">sent</span><span class="p">)</span> <span class="k">for</span> <span class="n">sent</span> <span class="ow">in</span> <span class="n">batch</span><span class="p">])</span>
        <span class="k">for</span> <span class="n">sent</span> <span class="ow">in</span> <span class="n">batch</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">maxlen</span> <span class="o">-</span> <span class="nf">len</span><span class="p">(</span><span class="n">sent</span><span class="p">)):</span>
                <span class="n">sent</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">pad_symbol</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">batch</span>
</pre></td> </tr></tbody></table></code></pre></figure> <p>Notice the two special tokens <code class="language-plaintext highlighter-rouge">&lt;SOS&gt;</code> and <code class="language-plaintext highlighter-rouge">&lt;EOS&gt;</code>, which represent start-of-sentence and end-of-sentence respectively. There are two other special tokens, <code class="language-plaintext highlighter-rouge">&lt;UNK&gt;</code>, which represents unknown tokens out of the vocabulary learned in this process, and <code class="language-plaintext highlighter-rouge">&lt;PAD&gt;</code>, which is used in the <code class="language-plaintext highlighter-rouge">pad()</code> method. Due to the architecture of <code class="language-plaintext highlighter-rouge">LSTMs</code> and similar language models, the number of tokens passed as input has to be the same. Since tweets can clearly be of variable length, we <code class="language-plaintext highlighter-rouge">pad()</code> the input using the <code class="language-plaintext highlighter-rouge">&lt;PAD&gt;</code> token that is removed again later in the process.</p> <h2 id="glove-embeddings">GloVe Embeddings</h2> <p>After initializing our tokenizer, we now have a way to convert them tidily into embeddings. Embeddings are vector representations of tokens that can be read and computed by the language model. While embeddings can simply be an index of the tokens i.e. <code class="language-plaintext highlighter-rouge">a = 1</code>, <code class="language-plaintext highlighter-rouge">ab = 2</code>, <code class="language-plaintext highlighter-rouge">an = 3</code>, etc., this throws out any information that can be gleamed from the tokens themselves. Consider the tokens <code class="language-plaintext highlighter-rouge">bio</code> and <code class="language-plaintext highlighter-rouge">logy</code>: both tokens clearly have some inherent meaning and co-occur at much greater probabilities than with other tokens like <code class="language-plaintext highlighter-rouge">sent</code>.</p> <p>Instead of indexing or randomly initializing our embeddings, we use <code class="language-plaintext highlighter-rouge">GloVe</code>, a set of embeddings that are learned from large language corpuses and capture these statistical relations. <code class="language-plaintext highlighter-rouge">GloVe</code> converts tokens into vectors and is available at different sizes from <code class="language-plaintext highlighter-rouge">25d</code> to <code class="language-plaintext highlighter-rouge">200d</code>. We’ll use the <code class="language-plaintext highlighter-rouge">200d</code> embeddings, which means that each token will be represented by <code class="language-plaintext highlighter-rouge">200</code> values where each value correlates the token to all the other token values on that dimension.</p> <p>There are approximately 1.2 million tokens represented in the <code class="language-plaintext highlighter-rouge">GloVe</code> file, which is certainly orders of magnitude greater than what we’ll encounter in the data. To save on compute, let’s create a custom embedding layer by learning a vocabulary using our training set:</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr>
<td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
</pre></td> <td class="code"><pre><span class="kn">from</span> <span class="n">functions.util</span> <span class="kn">import</span> <span class="n">load_datasets</span><span class="p">,</span> <span class="n">split_data</span>

<span class="c1"># Set up paths and constants
</span><span class="n">embeddings_path</span> <span class="o">=</span> <span class="sh">'</span><span class="s">glove.twitter.27B.200d.txt</span><span class="sh">'</span>
<span class="n">vocab_path</span> <span class="o">=</span> <span class="sh">"</span><span class="s">./vocab.txt</span><span class="sh">"</span>
<span class="n">SPECIAL_TOKENS</span> <span class="o">=</span> <span class="p">[</span><span class="sh">'</span><span class="s">&lt;UNK&gt;</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">&lt;PAD&gt;</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">&lt;SOS&gt;</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">&lt;EOS&gt;</span><span class="sh">'</span><span class="p">]</span>

<span class="c1"># Download and split data
</span><span class="n">train_sentences</span><span class="p">,</span> <span class="n">train_labels</span><span class="p">,</span> <span class="n">test_sentences</span><span class="p">,</span> <span class="n">test_labels</span><span class="p">,</span> <span class="n">label2i</span> <span class="o">=</span> <span class="nf">load_datasets</span><span class="p">()</span>
<span class="n">training_sentences</span><span class="p">,</span> <span class="n">training_labels</span><span class="p">,</span> <span class="n">dev_sentences</span><span class="p">,</span> <span class="n">dev_labels</span> <span class="o">=</span> <span class="nf">split_data</span><span class="p">(</span><span class="n">train_sentences</span><span class="p">,</span> <span class="n">train_labels</span><span class="p">,</span> <span class="n">split</span><span class="o">=</span><span class="mf">0.85</span><span class="p">)</span>

<span class="c1"># Set up tokenizer and make vocab
</span><span class="n">tokenizer</span> <span class="o">=</span> <span class="nc">Tokenizer</span><span class="p">()</span>
<span class="n">all_data</span> <span class="o">=</span> <span class="n">train_sentences</span> <span class="o">+</span> <span class="n">test_sentences</span>
<span class="n">tokenized_data</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">.</span><span class="nf">tokenize</span><span class="p">(</span><span class="n">all_data</span><span class="p">)</span>
<span class="n">vocab</span> <span class="o">=</span> <span class="nf">sorted</span><span class="p">(</span><span class="nf">set</span><span class="p">([</span><span class="n">w</span> <span class="k">for</span> <span class="n">ws</span> <span class="ow">in</span> <span class="n">tokenized_data</span> <span class="o">+</span> <span class="p">[</span><span class="n">SPECIAL_TOKENS</span><span class="p">]</span> <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">ws</span><span class="p">]))</span>
<span class="k">with</span> <span class="nf">open</span><span class="p">(</span><span class="sh">'</span><span class="s">vocab.txt</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">w</span><span class="sh">'</span><span class="p">)</span> <span class="k">as</span> <span class="n">vf</span><span class="p">:</span>
    <span class="n">vf</span><span class="p">.</span><span class="nf">write</span><span class="p">(</span><span class="sh">'</span><span class="se">\n</span><span class="sh">'</span><span class="p">.</span><span class="nf">join</span><span class="p">(</span><span class="n">vocab</span><span class="p">))</span>
</pre></td> </tr></tbody></table></code></pre></figure> <p>Once we’ve loaded in our vocabulary, there will certainly be some tokens that are not in <code class="language-plaintext highlighter-rouge">GloVe</code> from our <code class="language-plaintext highlighter-rouge">spaCy</code> tokenizer. We’ll have to randomly initialize these tokens which means that none of the semantic meaning will be captured in the embeddings. The best we can do in this case is to use <strong><a href="https://pytorch.org/docs/stable/_modules/torch/nn/init.html#xavier_uniform_" rel="external nofollow noopener" target="_blank">Xavier initialization</a></strong>, which creates vectors with values in the <code class="language-plaintext highlighter-rouge">normal distribution</code>:</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr>
<td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
59
60
61
62
63
64
65
66
67
68
69
70
71
72
73
74
75
76
77
78
79
80
81
82
83
84
85
86
87
88
89
90
91
92
93
94
95
96
97
98
99
100
101
</pre></td> <td class="code"><pre><span class="kn">from</span> <span class="n">typing</span> <span class="kn">import</span> <span class="n">List</span><span class="p">,</span> <span class="n">Dict</span><span class="p">,</span> <span class="n">Tuple</span>
<span class="kn">import</span> <span class="n">torch</span>


<span class="k">def</span> <span class="nf">read_pretrained_embeddings</span><span class="p">(</span>
    <span class="n">embeddings_path</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
    <span class="n">vocab_path</span><span class="p">:</span> <span class="nb">str</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">int</span><span class="p">],</span> <span class="n">torch</span><span class="p">.</span><span class="n">FloatTensor</span><span class="p">]:</span>
    <span class="sh">"""</span><span class="s">Read the embeddings matrix and make a dict hashing each word.
    Note that we have provided the entire vocab for train and test, so that for practical purposes
    we can simply load those words in the vocab, rather than all 27B embeddings.
    Args:
        embeddings_path (str): Glove embeddings downloaded by wget, pretrained run on 200d.
        vocab_path (str): Vocab file created from tokenizer.
    Returns:
        Tuple[Dict[str, int], torch.FloatTensor]: One hot vector of vocab, embeddings tensor.
    </span><span class="sh">"""</span>
    <span class="n">word2i</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="n">vectors</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">with</span> <span class="nf">open</span><span class="p">(</span><span class="n">vocab_path</span><span class="p">,</span> <span class="n">encoding</span><span class="o">=</span><span class="sh">'</span><span class="s">utf8</span><span class="sh">'</span><span class="p">)</span> <span class="k">as</span> <span class="n">vf</span><span class="p">:</span>
        <span class="n">vocab</span> <span class="o">=</span> <span class="nf">set</span><span class="p">([</span><span class="n">w</span><span class="p">.</span><span class="nf">strip</span><span class="p">()</span> <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">vf</span><span class="p">.</span><span class="nf">readlines</span><span class="p">()])</span> 
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Reading embeddings from </span><span class="si">{</span><span class="n">embeddings_path</span><span class="si">}</span><span class="s">...</span><span class="sh">"</span><span class="p">)</span>
    <span class="k">with</span> <span class="nf">open</span><span class="p">(</span><span class="n">embeddings_path</span><span class="p">,</span> <span class="sh">"</span><span class="s">r</span><span class="sh">"</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
        <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">for</span> <span class="n">line</span> <span class="ow">in</span> <span class="n">f</span><span class="p">:</span>
            <span class="n">word</span><span class="p">,</span> <span class="o">*</span><span class="n">weights</span> <span class="o">=</span> <span class="n">line</span><span class="p">.</span><span class="nf">rstrip</span><span class="p">().</span><span class="nf">split</span><span class="p">(</span><span class="sh">"</span><span class="s"> </span><span class="sh">"</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">vocab</span><span class="p">:</span>
                <span class="n">word2i</span><span class="p">[</span><span class="n">word</span><span class="p">]</span> <span class="o">=</span> <span class="n">i</span>
                <span class="n">vectors</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nc">Tensor</span><span class="p">([</span><span class="nf">float</span><span class="p">(</span><span class="n">w</span><span class="p">)</span> <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">weights</span><span class="p">]))</span>
                <span class="n">i</span> <span class="o">+=</span> <span class="mi">1</span>
    
    <span class="k">return</span> <span class="n">word2i</span><span class="p">,</span> <span class="n">torch</span><span class="p">.</span><span class="nf">stack</span><span class="p">(</span><span class="n">vectors</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">get_glove_embeddings</span><span class="p">(</span>
    <span class="n">embeddings_path</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">int</span><span class="p">],</span> <span class="n">torch</span><span class="p">.</span><span class="n">FloatTensor</span><span class="p">]:</span>
    <span class="sh">"""</span><span class="s">Read all glove embeddings and make a dict hashing each word instead.
    Args:
        embeddings_path (str): Glove embeddings downloaded by wget, pretrained run on 200d.
    Returns:
        Tuple[Dict[str, int], torch.FloatTensor]: One hot vector of vocab, embeddings tensor.
    </span><span class="sh">"""</span>
    <span class="n">word2i</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="n">vectors</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Reading embeddings from </span><span class="si">{</span><span class="n">embeddings_path</span><span class="si">}</span><span class="s">...</span><span class="sh">"</span><span class="p">)</span>
    <span class="k">with</span> <span class="nf">open</span><span class="p">(</span><span class="n">embeddings_path</span><span class="p">,</span> <span class="sh">"</span><span class="s">r</span><span class="sh">"</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
        <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">for</span> <span class="n">line</span> <span class="ow">in</span> <span class="n">f</span><span class="p">:</span>
            <span class="n">word</span><span class="p">,</span> <span class="o">*</span><span class="n">weights</span> <span class="o">=</span> <span class="n">line</span><span class="p">.</span><span class="nf">rstrip</span><span class="p">().</span><span class="nf">split</span><span class="p">(</span><span class="sh">"</span><span class="s"> </span><span class="sh">"</span><span class="p">)</span>
            <span class="n">word2i</span><span class="p">[</span><span class="n">word</span><span class="p">]</span> <span class="o">=</span> <span class="n">i</span>
            <span class="n">vectors</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nc">Tensor</span><span class="p">([</span><span class="nf">float</span><span class="p">(</span><span class="n">w</span><span class="p">)</span> <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">weights</span><span class="p">]))</span>
            <span class="n">i</span> <span class="o">+=</span> <span class="mi">1</span>

    <span class="k">return</span> <span class="n">word2i</span><span class="p">,</span> <span class="n">torch</span><span class="p">.</span><span class="nf">stack</span><span class="p">(</span><span class="n">vectors</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">get_oovs</span><span class="p">(</span><span class="n">vocab_path</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">word2i</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">int</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]:</span>
    <span class="sh">"""</span><span class="s">Find the vocab items that do not exist in the glove embeddings (in word2i).
    Return the List of such (unique) words.
    Args:
        vocab_path: List of batches of sentences.
        word2i (Dict[str, int]): See above.
    Returns:
        List[str]: Words not in glove.
    </span><span class="sh">"""</span>
    <span class="k">with</span> <span class="nf">open</span><span class="p">(</span><span class="n">vocab_path</span><span class="p">,</span> <span class="n">encoding</span><span class="o">=</span><span class="sh">'</span><span class="s">utf8</span><span class="sh">'</span><span class="p">)</span> <span class="k">as</span> <span class="n">vf</span><span class="p">:</span>
        <span class="n">vocab</span> <span class="o">=</span> <span class="nf">set</span><span class="p">([</span><span class="n">w</span><span class="p">.</span><span class="nf">strip</span><span class="p">()</span> <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">vf</span><span class="p">.</span><span class="nf">readlines</span><span class="p">()])</span>
    <span class="n">glove_and_vocab</span> <span class="o">=</span> <span class="nf">set</span><span class="p">(</span><span class="n">word2i</span><span class="p">.</span><span class="nf">keys</span><span class="p">())</span>
    <span class="n">vocab_and_not_glove</span> <span class="o">=</span> <span class="n">vocab</span> <span class="o">-</span> <span class="n">glove_and_vocab</span>
    
    <span class="k">return</span> <span class="nf">list</span><span class="p">(</span><span class="n">vocab_and_not_glove</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">intialize_new_embedding_weights</span><span class="p">(</span><span class="n">num_embeddings</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">dim</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="p">.</span><span class="n">FloatTensor</span><span class="p">:</span>
    <span class="sh">"""</span><span class="s">Xavier initialization for the embeddings of words in train, but not in glove.</span><span class="sh">"""</span>
    <span class="n">weights</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">empty</span><span class="p">(</span><span class="n">num_embeddings</span><span class="p">,</span> <span class="n">dim</span><span class="p">)</span>
    <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">init</span><span class="p">.</span><span class="nf">xavier_uniform_</span><span class="p">(</span><span class="n">weights</span><span class="p">)</span>
    
    <span class="k">return</span> <span class="n">weights</span>


<span class="k">def</span> <span class="nf">update_embeddings</span><span class="p">(</span>
    <span class="n">glove_word2i</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">int</span><span class="p">],</span>
    <span class="n">glove_embeddings</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">FloatTensor</span><span class="p">,</span>
    <span class="n">oovs</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">int</span><span class="p">],</span> <span class="n">torch</span><span class="p">.</span><span class="n">FloatTensor</span><span class="p">]:</span>
    <span class="n">i</span> <span class="o">=</span> <span class="nf">len</span><span class="p">(</span><span class="n">glove_word2i</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">oov</span> <span class="ow">in</span> <span class="n">oovs</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">oov</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">glove_word2i</span><span class="p">:</span>
            <span class="n">glove_word2i</span><span class="p">[</span><span class="n">oov</span><span class="p">]</span> <span class="o">=</span> <span class="n">i</span>
            <span class="n">i</span> <span class="o">+=</span> <span class="mi">1</span>
    <span class="n">new_embeddings</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">cat</span><span class="p">((</span><span class="n">glove_embeddings</span><span class="p">,</span> <span class="nf">intialize_new_embedding_weights</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">oovs</span><span class="p">),</span> <span class="n">glove_embeddings</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])))</span>
    
    <span class="k">return</span> <span class="n">glove_word2i</span><span class="p">,</span> <span class="n">new_embeddings</span>


<span class="c1"># Load the pretrained embeddings, find the out-of-vocabularies, and add to word2i and embeddings
</span><span class="n">glove_word2i</span><span class="p">,</span> <span class="n">glove_embeddings</span> <span class="o">=</span> <span class="nf">read_pretrained_embeddings</span><span class="p">(</span><span class="n">embeddings_path</span><span class="p">,</span> <span class="n">vocab_path</span><span class="p">)</span>
<span class="n">oovs</span> <span class="o">=</span> <span class="nf">get_oovs</span><span class="p">(</span><span class="n">vocab_path</span><span class="p">,</span> <span class="n">glove_word2i</span><span class="p">)</span>
<span class="n">word2i</span><span class="p">,</span> <span class="n">embeddings</span> <span class="o">=</span> <span class="nf">update_embeddings</span><span class="p">(</span><span class="n">glove_word2i</span><span class="p">,</span> <span class="n">glove_embeddings</span><span class="p">,</span> <span class="n">oovs</span><span class="p">)</span>
</pre></td> </tr></tbody></table></code></pre></figure> <p>Notice in the execution step we’ve developed a <code class="language-plaintext highlighter-rouge">word2i</code> dictionary, a data structure that indexes all the tokens in our vocabulary to their corresponding <code class="language-plaintext highlighter-rouge">GloVe</code> embedding.</p> <h2 id="batching-and-encoding">Batching and Encoding</h2> <div class="row justify-content-md-center"> <div class="col-sm-auto mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/projects/irony_one-hot-480.webp 480w,/assets/img/projects/irony_one-hot-800.webp 800w,/assets/img/projects/irony_one-hot-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/projects/irony_one-hot.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="one-hot vector encoding" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p>Now that we’ve got a vocabulary and a set of embeddings, we’ll need to encode our inputs and batch them for the training loop. Encoding is a simple and useful preprocessing step that helps us quickly convert our tokens into their respective embeddings. We use the common <strong><a href="https://en.wikipedia.org/wiki/One-hot" rel="external nofollow noopener" target="_blank">one-hot vector</a></strong> method, which gives each token its unique index. The token are converted into vectors with the length of the vocabulary which has a value <code class="language-plaintext highlighter-rouge">1</code> at the index and <code class="language-plaintext highlighter-rouge">0</code> otherwise.</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr>
<td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
</pre></td> <td class="code"><pre><span class="kn">from</span> <span class="n">typing</span> <span class="kn">import</span> <span class="n">List</span><span class="p">,</span> <span class="n">Dict</span>
<span class="kn">import</span> <span class="n">torch</span>

<span class="k">def</span> <span class="nf">encode_sentences</span><span class="p">(</span><span class="n">batch</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]],</span> <span class="n">word2i</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">int</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="p">.</span><span class="n">LongTensor</span><span class="p">:</span>
    <span class="sh">"""</span><span class="s">Encode the tokens in each sentence in the batch with a dictionary.
    Args:
        batch (List[List[str]]): The padded and tokenized batch of sentences.
        word2i (Dict[str, int]): The encoding dictionary.
    Returns:
        torch.LongTensor: The tensor of encoded sentences.
    </span><span class="sh">"""</span>
    <span class="n">UNK_IDX</span> <span class="o">=</span> <span class="n">word2i</span><span class="p">[</span><span class="sh">"</span><span class="s">&lt;UNK&gt;</span><span class="sh">"</span><span class="p">]</span>
    <span class="n">tensors</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">sent</span> <span class="ow">in</span> <span class="n">batch</span><span class="p">:</span>
        <span class="n">tensors</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nc">LongTensor</span><span class="p">([</span><span class="n">word2i</span><span class="p">.</span><span class="nf">get</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">UNK_IDX</span><span class="p">)</span> <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">sent</span><span class="p">]))</span>
        
    <span class="k">return</span> <span class="n">torch</span><span class="p">.</span><span class="nf">stack</span><span class="p">(</span><span class="n">tensors</span><span class="p">)</span>
</pre></td> </tr></tbody></table></code></pre></figure> <p>The <code class="language-plaintext highlighter-rouge">encode_sentences()</code> method returns a matrix where each row represents the one-hot vector of the input token. When this matrix is multiplied with the embeddings matrix, we extract all the proper embeddings with great efficiency through the <code class="language-plaintext highlighter-rouge">PyTorch</code> implementation of matrix multiplication instead of other more costly functions.</p> <p>The corresponding <code class="language-plaintext highlighter-rouge">encode_labels()</code> function is simply to turn our labels into a tensor:</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr>
<td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
</pre></td> <td class="code"><pre> <span class="k">def</span> <span class="nf">encode_labels</span><span class="p">(</span><span class="n">labels</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="p">.</span><span class="n">FloatTensor</span><span class="p">:</span>
    <span class="sh">"""</span><span class="s">Turns the batch of labels into a tensor.
    Args:
        labels (List[int]): List of all labels in the batch.
    Returns:
        torch.FloatTensor: Tensor of all labels in the batch.
    </span><span class="sh">"""</span>
    <span class="k">return</span> <span class="n">torch</span><span class="p">.</span><span class="nc">LongTensor</span><span class="p">([</span><span class="nf">int</span><span class="p">(</span><span class="n">l</span><span class="p">)</span> <span class="k">for</span> <span class="n">l</span> <span class="ow">in</span> <span class="n">labels</span><span class="p">])</span>
</pre></td> </tr></tbody></table></code></pre></figure> <p>The next part of the process is to <code class="language-plaintext highlighter-rouge">batch</code> our inputs for training, which is simply to run multiple samples through the training loop before updating our weights. We introduce the functions here, but since <code class="language-plaintext highlighter-rouge">batch_size</code> is an important hyperparameter both batching and encoding will be placed after our model is initialized.</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr>
<td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
</pre></td> <td class="code"><pre> <span class="k">def</span> <span class="nf">make_batches</span><span class="p">(</span><span class="n">sequences</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">],</span> <span class="n">batch_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]]:</span>
    <span class="sh">"""</span><span class="s">Yield batch_size chunks from sequences.</span><span class="sh">"""</span>
    <span class="n">batches</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="nf">len</span><span class="p">(</span><span class="n">sequences</span><span class="p">),</span> <span class="n">batch_size</span><span class="p">):</span>
        <span class="n">batches</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">sequences</span><span class="p">[</span><span class="n">i</span><span class="p">:</span><span class="n">i</span> <span class="o">+</span> <span class="n">batch_size</span><span class="p">])</span>
    
    <span class="k">return</span> <span class="n">batches</span>
</pre></td> </tr></tbody></table></code></pre></figure> <h2 id="modeling">Modeling</h2> <p>All the pieces are in place for us to implement our model in <code class="language-plaintext highlighter-rouge">PyTorch</code>, a powerful yet easy to use library for neural networks:</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr>
<td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
</pre></td> <td class="code"><pre><span class="kn">import</span> <span class="n">torch</span>

<span class="k">class</span> <span class="nc">IronyDetector</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span>
        <span class="n">self</span><span class="p">,</span>
        <span class="n">input_dim</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">hidden_dim</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">embeddings_tensor</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">FloatTensor</span><span class="p">,</span>
        <span class="n">pad_idx</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">output_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">dropout_val</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.5</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">input_dim</span> <span class="o">=</span> <span class="n">input_dim</span>
        <span class="n">self</span><span class="p">.</span><span class="n">hidden_dim</span> <span class="o">=</span> <span class="n">hidden_dim</span>
        <span class="n">self</span><span class="p">.</span><span class="n">pad_idx</span> <span class="o">=</span> <span class="n">pad_idx</span>
        <span class="n">self</span><span class="p">.</span><span class="n">dropout_val</span> <span class="o">=</span> <span class="n">dropout_val</span>
        <span class="n">self</span><span class="p">.</span><span class="n">output_size</span> <span class="o">=</span> <span class="n">output_size</span>
        <span class="c1"># Initialize the embeddings from the weights matrix.
</span>        <span class="n">self</span><span class="p">.</span><span class="n">embeddings</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">Embedding</span><span class="p">.</span><span class="nf">from_pretrained</span><span class="p">(</span><span class="n">embeddings_tensor</span><span class="p">,</span> <span class="bp">True</span><span class="p">,</span> <span class="n">pad_idx</span><span class="p">)</span>
        <span class="c1"># Dropout regularization
</span>        <span class="n">self</span><span class="p">.</span><span class="n">dropout_layer</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="nc">Dropout</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">dropout_val</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
        <span class="c1"># 2-layer Bi-LSTM
</span>        <span class="n">self</span><span class="p">.</span><span class="n">lstm</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="nc">LSTM</span><span class="p">(</span>
            <span class="n">self</span><span class="p">.</span><span class="n">input_dim</span><span class="p">,</span>
            <span class="n">self</span><span class="p">.</span><span class="n">hidden_dim</span><span class="p">,</span>
            <span class="n">num_layers</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>
            <span class="n">dropout</span><span class="o">=</span><span class="n">dropout_val</span><span class="p">,</span>
            <span class="n">batch_first</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
            <span class="n">bidirectional</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="c1"># For classification over the final LSTM state.
</span>        <span class="n">self</span><span class="p">.</span><span class="n">classifier</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">hidden_dim</span><span class="o">*</span><span class="mi">2</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">output_size</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">log_softmax</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="nc">LogSoftmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
</pre></td> </tr></tbody></table></code></pre></figure> <p>By using the <code class="language-plaintext highlighter-rouge">super()</code> method, we can initialize a custom model that inherits all its functions from <a href="https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module" rel="external nofollow noopener" target="_blank"><code class="language-plaintext highlighter-rouge">torch.nn.Module</code></a>. Since we’re using the custom embedding layer we’ve built, we’ll need to add them to the module with the convenient <a href="https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html#torch.nn.Embedding.from_pretrained" rel="external nofollow noopener" target="_blank"><code class="language-plaintext highlighter-rouge">self.embeddings = torch.nn.Embedding.from_pretrained(embeddings_tensor, True, pad_idx)</code></a>. A hyperparameter, the <a href="https://jmlr.org/papers/v15/srivastava14a.html" rel="external nofollow noopener" target="_blank">dropout value</a> is also initialized with our model and can be adjusted before the training portion.</p> <p>With all of the structure abstracted away, we’ve initialized a <code class="language-plaintext highlighter-rouge">BiLSTM</code> network with 4 components: an embedding layer that converts tokens into embeddings, a <code class="language-plaintext highlighter-rouge">BiLSTM</code> layer that converts embedding vectors into a vector representing the whole input with respect to the classification task, a <code class="language-plaintext highlighter-rouge">feedforward</code> layer using the <a href="https://pytorch.org/docs/stable/generated/torch.nn.Linear.html" rel="external nofollow noopener" target="_blank"><code class="language-plaintext highlighter-rouge">torch.nn.Linear</code></a> class, and finally the <code class="language-plaintext highlighter-rouge">softmax</code> layer that performs the final classification step.</p> <h2 id="training">Training</h2> <p>The training loop is fairly straightforward since <code class="language-plaintext highlighter-rouge">torch.nn.Module</code> comes with built-in forward and backprop functionality. Although Task A is a binary classification task, Task B is multiclass, so we use the <code class="language-plaintext highlighter-rouge">torch.nn.NLLLoss()</code> function for the loss calculation and backpropagation.</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr>
<td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
</pre></td> <td class="code"><pre><span class="kn">from</span> <span class="n">tqdm.notebook</span> <span class="kn">import</span> <span class="n">tqdm</span> <span class="k">as</span> <span class="n">tqdm</span>
<span class="kn">from</span> <span class="n">functions.util</span> <span class="kn">import</span> <span class="n">f1_score</span><span class="p">,</span> <span class="n">avg_f1_score</span>
<span class="kn">import</span> <span class="n">random</span>
<span class="kn">import</span> <span class="n">torch</span>

<span class="k">def</span> <span class="nf">training_loop</span><span class="p">(</span>
    <span class="n">num_epochs</span><span class="p">,</span>
    <span class="n">train_features</span><span class="p">,</span>
    <span class="n">train_labels</span><span class="p">,</span>
    <span class="n">dev_features</span><span class="p">,</span>
    <span class="n">dev_labels</span><span class="p">,</span>
    <span class="n">optimizer</span><span class="p">,</span>
    <span class="n">model</span><span class="p">,</span>
    <span class="n">label2i</span><span class="p">,</span>
<span class="p">):</span>
    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Training...</span><span class="sh">"</span><span class="p">)</span>
    <span class="n">loss_func</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="nc">NLLLoss</span><span class="p">()</span>
    <span class="n">batches</span> <span class="o">=</span> <span class="nf">list</span><span class="p">(</span><span class="nf">zip</span><span class="p">(</span><span class="n">train_features</span><span class="p">,</span> <span class="n">train_labels</span><span class="p">))</span>
    <span class="n">random</span><span class="p">.</span><span class="nf">shuffle</span><span class="p">(</span><span class="n">batches</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">num_epochs</span><span class="p">):</span>
        <span class="n">losses</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">features</span><span class="p">,</span> <span class="n">labels</span> <span class="ow">in</span> <span class="nf">tqdm</span><span class="p">(</span><span class="n">batches</span><span class="p">):</span>
            <span class="c1"># Empty the dynamic computation graph
</span>            <span class="n">optimizer</span><span class="p">.</span><span class="nf">zero_grad</span><span class="p">()</span>
            <span class="n">logits</span> <span class="o">=</span> <span class="nf">model</span><span class="p">(</span><span class="n">features</span><span class="p">).</span><span class="nf">squeeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
            <span class="n">loss</span> <span class="o">=</span> <span class="nf">loss_func</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span>
            <span class="c1"># Backpropogate the loss through our model
</span>            <span class="n">loss</span><span class="p">.</span><span class="nf">backward</span><span class="p">()</span>
            <span class="n">optimizer</span><span class="p">.</span><span class="nf">step</span><span class="p">()</span>
            <span class="n">losses</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">loss</span><span class="p">.</span><span class="nf">item</span><span class="p">())</span>
        <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">epoch </span><span class="si">{</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s">, loss: </span><span class="si">{</span><span class="nf">sum</span><span class="p">(</span><span class="n">losses</span><span class="p">)</span><span class="o">/</span><span class="nf">len</span><span class="p">(</span><span class="n">losses</span><span class="p">)</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
        <span class="c1"># Estimate the f1 score for the development set
</span>        <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Evaluating dev...</span><span class="sh">"</span><span class="p">)</span>
        <span class="n">preds</span> <span class="o">=</span> <span class="nf">predict</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">dev_features</span><span class="p">)</span>
        <span class="n">dev_f1</span> <span class="o">=</span> <span class="nf">f1_score</span><span class="p">(</span><span class="n">preds</span><span class="p">,</span> <span class="n">dev_labels</span><span class="p">,</span> <span class="n">label2i</span><span class="p">[</span><span class="sh">'</span><span class="s">1</span><span class="sh">'</span><span class="p">])</span>
        <span class="n">dev_avg_f1</span> <span class="o">=</span> <span class="nf">avg_f1_score</span><span class="p">(</span><span class="n">preds</span><span class="p">,</span> <span class="n">dev_labels</span><span class="p">,</span> <span class="nf">set</span><span class="p">(</span><span class="n">label2i</span><span class="p">.</span><span class="nf">values</span><span class="p">()))</span>
        <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Dev F1 </span><span class="si">{</span><span class="n">dev_f1</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
        <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Avg Dev F1 </span><span class="si">{</span><span class="n">dev_avg_f1</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
    <span class="c1"># Return the trained model
</span>    <span class="k">return</span> <span class="n">model</span>
</pre></td> </tr></tbody></table></code></pre></figure> <p>Putting everything together let’s first initialize our model:</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr>
<td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
</pre></td> <td class="code"><pre> <span class="n">model</span> <span class="o">=</span> <span class="nc">IronyDetector</span><span class="p">(</span>
    <span class="n">input_dim</span><span class="o">=</span><span class="n">embeddings</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span>
    <span class="n">hidden_dim</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span>
    <span class="n">embeddings_tensor</span><span class="o">=</span><span class="n">embeddings</span><span class="p">,</span>
    <span class="n">pad_idx</span><span class="o">=</span><span class="n">word2i</span><span class="p">[</span><span class="sh">"</span><span class="s">&lt;PAD&gt;</span><span class="sh">"</span><span class="p">],</span>
    <span class="n">output_size</span><span class="o">=</span><span class="nf">len</span><span class="p">(</span><span class="n">label2i</span><span class="p">),</span>
<span class="p">)</span>
</pre></td> </tr></tbody></table></code></pre></figure> <p>We then set our hyperparameters including the <a href="https://pytorch.org/docs/stable/optim.html" rel="external nofollow noopener" target="_blank"><code class="language-plaintext highlighter-rouge">optimizer</code></a>, many of which are prebuilt in <code class="language-plaintext highlighter-rouge">PyTorch</code>. I’ve gotten the best results with <code class="language-plaintext highlighter-rouge">AdamW</code> which is what I’ve shown here:</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr>
<td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
</pre></td> <td class="code"><pre><span class="c1"># Hyperparameters
</span><span class="n">batch_size</span> <span class="o">=</span> <span class="mi">8</span>
<span class="n">epochs</span> <span class="o">=</span> <span class="mi">15</span>
<span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">0.00005</span>
<span class="n">weight_decay</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">optim</span><span class="p">.</span><span class="nc">AdamW</span><span class="p">(</span><span class="n">model</span><span class="p">.</span><span class="nf">parameters</span><span class="p">(),</span> <span class="n">learning_rate</span><span class="p">,</span> <span class="n">weight_decay</span><span class="o">=</span><span class="n">weight_decay</span><span class="p">)</span>
</pre></td> </tr></tbody></table></code></pre></figure> <p>Having defined <code class="language-plaintext highlighter-rouge">batch_size</code> we can now batch and encode our inputs:</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr>
<td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
</pre></td> <td class="code"><pre><span class="c1"># Create batches
</span><span class="n">batch_tokenized</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">batch</span> <span class="ow">in</span> <span class="nf">make_batches</span><span class="p">(</span><span class="n">training_sentences</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">):</span>
    <span class="n">batch_tokenized</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="nf">tokenizer</span><span class="p">(</span><span class="n">batch</span><span class="p">))</span>
<span class="n">batch_labels</span> <span class="o">=</span> <span class="nf">make_batches</span><span class="p">(</span><span class="n">training_labels</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">)</span>
<span class="n">dev_sentences</span> <span class="o">=</span> <span class="nf">tokenizer</span><span class="p">(</span><span class="n">dev_sentences</span><span class="p">)</span>
<span class="n">test_sentences</span> <span class="o">=</span> <span class="nf">tokenizer</span><span class="p">(</span><span class="n">test_sentences</span><span class="p">)</span>

<span class="c1"># Encode data
</span><span class="n">train_features</span> <span class="o">=</span> <span class="p">[</span><span class="nf">encode_sentences</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">word2i</span><span class="p">)</span> <span class="k">for</span> <span class="n">batch</span> <span class="ow">in</span> <span class="n">batch_tokenized</span><span class="p">]</span>
<span class="n">train_labels</span> <span class="o">=</span> <span class="p">[</span><span class="nf">encode_labels</span><span class="p">(</span><span class="n">batch</span><span class="p">)</span> <span class="k">for</span> <span class="n">batch</span> <span class="ow">in</span> <span class="n">batch_labels</span><span class="p">]</span>
<span class="n">dev_features</span> <span class="o">=</span> <span class="nf">encode_sentences</span><span class="p">(</span><span class="n">dev_sentences</span><span class="p">,</span> <span class="n">word2i</span><span class="p">)</span>
<span class="n">dev_labels</span> <span class="o">=</span> <span class="p">[</span><span class="nf">int</span><span class="p">(</span><span class="n">l</span><span class="p">)</span> <span class="k">for</span> <span class="n">l</span> <span class="ow">in</span> <span class="n">dev_labels</span><span class="p">]</span>
</pre></td> </tr></tbody></table></code></pre></figure> <p>Finally, let’s train the model!</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr>
<td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
</pre></td> <td class="code"><pre><span class="c1"># Train model
</span><span class="n">trained_model</span> <span class="o">=</span> <span class="nf">training_loop</span><span class="p">(</span>
    <span class="n">epochs</span><span class="p">,</span>
    <span class="n">train_features</span><span class="p">,</span>
    <span class="n">train_labels</span><span class="p">,</span>
    <span class="n">dev_features</span><span class="p">,</span>
    <span class="n">dev_labels</span><span class="p">,</span>
    <span class="n">optimizer</span><span class="p">,</span>
    <span class="n">model</span><span class="p">,</span>
    <span class="n">label2i</span><span class="p">,</span>
<span class="p">)</span>
</pre></td> </tr></tbody></table></code></pre></figure> <h2 id="results">Results</h2> <p>To test our model let’s write a simple predict functions:</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr>
<td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
</pre></td> <td class="code"><pre><span class="kn">from</span> <span class="n">typing</span> <span class="kn">import</span> <span class="n">List</span>
<span class="kn">from</span> <span class="n">functions.util</span> <span class="kn">import</span> <span class="n">f1_score</span><span class="p">,</span> <span class="n">avg_f1_score</span>
<span class="kn">import</span> <span class="n">torch</span>

<span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="n">model</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">,</span> <span class="n">sequences</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">]):</span>
    <span class="sh">"""</span><span class="s">Run prediction with the model
    Args: model (torch.nn.Module): Model to use.
        dev_sequences (List[torch.Tensor]): List of encoded sequences to analyze.
    Returns:
        List[int]: List of predicted labels.
    </span><span class="sh">"""</span>
    <span class="n">preds</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">with</span> <span class="n">torch</span><span class="p">.</span><span class="nf">no_grad</span><span class="p">():</span>
        <span class="n">model</span><span class="p">.</span><span class="nf">eval</span><span class="p">()</span>
        <span class="n">logits</span> <span class="o">=</span> <span class="nf">model</span><span class="p">(</span><span class="n">sequences</span><span class="p">)</span>
        <span class="n">preds</span> <span class="o">=</span> <span class="nf">list</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">argmax</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">2</span><span class="p">).</span><span class="nf">squeeze</span><span class="p">().</span><span class="nf">numpy</span><span class="p">())</span>
    <span class="k">return</span> <span class="n">preds</span>

<span class="c1"># Test model
</span><span class="n">test_features</span> <span class="o">=</span> <span class="nf">encode_sentences</span><span class="p">(</span><span class="n">test_sentences</span><span class="p">,</span> <span class="n">word2i</span><span class="p">)</span>
<span class="n">test_labels</span> <span class="o">=</span> <span class="p">[</span><span class="nf">int</span><span class="p">(</span><span class="n">l</span><span class="p">)</span> <span class="k">for</span> <span class="n">l</span> <span class="ow">in</span> <span class="n">test_labels</span><span class="p">]</span>
<span class="n">preds</span> <span class="o">=</span> <span class="nf">predict</span><span class="p">(</span><span class="n">trained_model</span><span class="p">,</span> <span class="n">test_features</span><span class="p">)</span>
<span class="n">dev_f1</span> <span class="o">=</span> <span class="nf">f1_score</span><span class="p">(</span><span class="n">preds</span><span class="p">,</span> <span class="n">test_labels</span><span class="p">,</span> <span class="n">label2i</span><span class="p">[</span><span class="sh">'</span><span class="s">1</span><span class="sh">'</span><span class="p">])</span>
<span class="n">dev_avg_f1</span> <span class="o">=</span> <span class="nf">avg_f1_score</span><span class="p">(</span><span class="n">preds</span><span class="p">,</span> <span class="n">test_labels</span><span class="p">,</span> <span class="nf">set</span><span class="p">(</span><span class="n">label2i</span><span class="p">.</span><span class="nf">values</span><span class="p">()))</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Test F1 </span><span class="si">{</span><span class="n">dev_f1</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Avg Test F1 </span><span class="si">{</span><span class="n">dev_avg_f1</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
</pre></td> </tr></tbody></table></code></pre></figure> <p>In this function, we use <code class="language-plaintext highlighter-rouge">with torch.no_grad()</code> so that no gradients are generated since we are not backpropagating anything here. We then set the model to evaluation mode with <code class="language-plaintext highlighter-rouge">model.eval()</code>, which freezes the weights and further prevents any sort of changes to the model during prediction. Since our outputs represent binary probabilities, we’ll need to <code class="language-plaintext highlighter-rouge">squeeze()</code> the tensor to get the correct shape.</p> <p>Here are some of the hyperparameters I experimented with:</p> <table> <thead> <tr> <th><strong>Test Avg F1</strong></th> <th><strong>Epochs</strong></th> <th><strong>Learning Rate</strong></th> <th><strong>Optimizer</strong></th> <th><strong>Hidden Layer Dimension</strong></th> <th><strong>Weight Decay</strong></th> </tr> </thead> <tbody> <tr> <td><strong>0.7000</strong></td> <td>20</td> <td>0.00005</td> <td>AdamW</td> <td>128</td> <td>0</td> </tr> <tr> <td><strong>0.6947</strong></td> <td>20</td> <td>0.00005</td> <td>AdamW</td> <td>256</td> <td>0</td> </tr> <tr> <td><strong>0.6947</strong></td> <td>15</td> <td>0.00005</td> <td>AdamW</td> <td>256</td> <td>0</td> </tr> <tr> <td><strong>0.6798</strong></td> <td>15</td> <td>0.00005</td> <td>AdamW</td> <td>128</td> <td>0.01</td> </tr> <tr> <td><strong>0.6657</strong></td> <td>15</td> <td>0.00005</td> <td>AdamW</td> <td>128</td> <td>0.1</td> </tr> <tr> <td><strong>0.6899</strong></td> <td>15</td> <td>0.0001</td> <td>AdamW</td> <td>64</td> <td>0.1</td> </tr> <tr> <td><strong>0.6525</strong></td> <td>15</td> <td>0.001</td> <td>AdamW</td> <td>64</td> <td>0.1</td> </tr> <tr> <td><strong>0.5535</strong></td> <td>10</td> <td>0.001</td> <td>AdamW</td> <td>64</td> <td>0.1</td> </tr> <tr> <td><strong>0.5556</strong></td> <td>10</td> <td>0.1</td> <td>SGD</td> <td>32</td> <td>0</td> </tr> <tr> <td><strong>0.2840</strong></td> <td>20</td> <td>0.00001</td> <td>RAdam</td> <td>128</td> <td>0.1</td> </tr> <tr> <td><strong>0.6846</strong></td> <td>15</td> <td>0.001</td> <td>RAdam</td> <td>64</td> <td>0.0001</td> </tr> <tr> <td><strong>0.3758</strong></td> <td>12</td> <td>0.01</td> <td>RAdam</td> <td>128</td> <td>0.0001</td> </tr> <tr> <td><strong>0.7129</strong></td> <td>10</td> <td>0.00005</td> <td>AdamW</td> <td>128</td> <td>0</td> </tr> </tbody> </table> <p>As discussed in the introduction, there really isn’t enough training data to train all the paramters for large hidden layer dimensions and peak performance was reached at <code class="language-plaintext highlighter-rouge">hidden_dim=128</code>. Similarly, at small learning rates there wasn’t enough signal to backpropagate through the model, which resulted in failed training loops. Weight decay was also an interesting hyperparameter to toy around with although I ultimately achieved best results by setting it to <code class="language-plaintext highlighter-rouge">0</code>.</p> <h2 id="sources">Sources</h2> <ul> <li> <a href="https://github.com/Cyvhee/SemEval2018-Task3" rel="external nofollow noopener" target="_blank">SemEval Repository</a> (<a href="https://competitions.codalab.org/competitions/17468#results" rel="external nofollow noopener" target="_blank">Codalab Competition</a>)</li> <li><a href="https://nlp.stanford.edu/projects/glove/" rel="external nofollow noopener" target="_blank">GloVe embeddings</a></li> <li><a href="https://pytorch.org/docs/stable/_modules/torch/nn/init.html#xavier_uniform_" rel="external nofollow noopener" target="_blank">Xavier Initialization</a></li> <li><a href="https://paperswithcode.com/method/bilstm" rel="external nofollow noopener" target="_blank">Understanding BiLSTMs</a></li> <li><a href="https://colah.github.io/posts/2015-08-Understanding-LSTMs/" rel="external nofollow noopener" target="_blank">Understanding LSTMs</a></li> <li><a href="https://www.geeksforgeeks.org/introduction-to-recurrent-neural-network/" rel="external nofollow noopener" target="_blank">Intro to RNNs</a></li> <li><a href="https://www.geeksforgeeks.org/self-supervised-learning-ssl/" rel="external nofollow noopener" target="_blank">Self-Supervised Learning</a></li> <li><a href="https://en.wikipedia.org/wiki/One-hot" rel="external nofollow noopener" target="_blank">One-Hot Vector Encoding</a></li> <li><a href="https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module" rel="external nofollow noopener" target="_blank"><code class="language-plaintext highlighter-rouge">torch.nn.Module</code> Documentation</a></li> <li><a href="https://jmlr.org/papers/v15/srivastava14a.html" rel="external nofollow noopener" target="_blank">Understanding Dropout Values</a></li> <li><a href="https://pytorch.org/docs/stable/generated/torch.nn.Linear.html" rel="external nofollow noopener" target="_blank"><code class="language-plaintext highlighter-rouge">torch.nn.Linear</code> Documentation</a></li> <li><a href="https://pytorch.org/docs/stable/optim.html" rel="external nofollow noopener" target="_blank"><code class="language-plaintext highlighter-rouge">torch.optim</code> Documentation</a></li> </ul> </article> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Alvin Chen 陳柏駿. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Last updated: July 15, 2025. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.min.js" integrity="sha256-rjmgmaB99riUNcdlrDtcAiwtLIojSxNyUFdl+Qh+rB4=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> <script type="module" src="/assets/js/search/ninja-keys.min.js?601a2d3465e2a52bec38b600518d5f70"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script>let searchTheme=determineComputedTheme();const ninjaKeys=document.querySelector("ninja-keys");"dark"===searchTheme?ninjaKeys.classList.add("dark"):ninjaKeys.classList.remove("dark");const openSearchModal=()=>{const e=$("#navbarNav");e.hasClass("show")&&e.collapse("hide"),ninjaKeys.open()};</script> <script>const ninja=document.querySelector("ninja-keys");ninja.data=[{id:"nav-about",title:"about",section:"Navigation",handler:()=>{window.location.href="/"}},{id:"nav-blog",title:"blog",description:"",section:"Navigation",handler:()=>{window.location.href="/blog/"}},{id:"nav-publications",title:"publications",description:"",section:"Navigation",handler:()=>{window.location.href="/publications/"}},{id:"nav-projects",title:"projects",description:"I update my projects once in a while.",section:"Navigation",handler:()=>{window.location.href="/projects/"}},{id:"nav-repositories",title:"repositories",description:"For detailed descriptions please view projects!",section:"Navigation",handler:()=>{window.location.href="/repositories/"}},{id:"nav-cv",title:"cv",description:"",section:"Navigation",handler:()=>{window.location.href="/cv/"}},{id:"news-lt-a-href-quot-https-lindat-mff-cuni-cz-repository-items-239427de-bcaa-401d-a0ae-2c69602daa67-quot-gt-the-umr-project-released-the-2-0-dataset-and-the-english-corpus-now-has-over-30k-sentences-lt-a-gt",title:"&lt;a href=&quot;https://lindat.mff.cuni.cz/repository/items/239427de-bcaa-401d-a0ae-2c69602daa67&quot;&gt;The UMR Project released the 2.0 dataset and the English corpus now has over 30k sentences!&lt;/a&gt;",description:"",section:"News"},{id:"news-lt-a-href-quot-projects-cnn-quot-gt-i-built-a-convolutional-neural-network-in-numpy-that-performs-at-tensorflow-levels-lt-a-gt",title:"&lt;a href=&quot;/projects/cnn/&quot;&gt;I built a Convolutional Neural Network in NumPy that performs at TensorFlow levels!&lt;/a&gt;",description:"",section:"News"},{id:"news-lt-a-href-quot-projects-irony-quot-gt-i-implemented-an-irony-detector-for-tweets-that-outperforms-sota-lt-a-gt",title:"&lt;a href=&quot;/projects/irony/&quot;&gt;I implemented an Irony Detector for tweets that outperforms SOTA!&lt;/a&gt;",description:"",section:"News"},{id:"projects-numpy-cnn",title:"numpy cnn",description:"A Convolutional Neural Network implemented in NumPy with modular layers.",section:"Projects",handler:()=>{window.location.href="/projects/cnn/"}},{id:"projects-irony-detector",title:"irony detector",description:"A BiLSTM that detects irony in tweets based off SemEval 2018 Task 3.",section:"Projects",handler:()=>{window.location.href="/projects/irony/"}},{id:"socials-email",title:"Send email",section:"Socials",handler:()=>{window.open("mailto:%61%6C%76%69%6E.%63%68%65%6E@%63%6F%6C%6F%72%61%64%6F.%65%64%75","_blank")}},{id:"socials-github",title:"GitHub",section:"Socials",handler:()=>{window.open("https://github.com/alvin-pc-chen","_blank")}},{id:"socials-linkedin",title:"LinkedIn",section:"Socials",handler:()=>{window.open("https://www.linkedin.com/in/alvin-pc-chen","_blank")}},{id:"socials-medium",title:"Medium",section:"Socials",handler:()=>{window.open("https://medium.com/@alch6097","_blank")}},{id:"socials-bluesky",title:"Bluesky",section:"Socials",handler:()=>{window.open("https://bsky.app/profile/https://bsky.app/profile/alvinchen.bsky.social","_blank")}},{id:"light-theme",title:"Change theme to light",description:"Change the theme of the site to Light",section:"Theme",handler:()=>{setThemeSetting("light")}},{id:"dark-theme",title:"Change theme to dark",description:"Change the theme of the site to Dark",section:"Theme",handler:()=>{setThemeSetting("dark")}},{id:"system-theme",title:"Use system default theme",description:"Change the theme of the site to System Default",section:"Theme",handler:()=>{setThemeSetting("system")}}];</script> <script src="/assets/js/shortcut-key.js"></script> </body> </html>