---
---

@misc{chen2024studyingeffectscollaborationinteractive,
  bibtex_show={true},
  title = {Studying the Effects of Collaboration in Interactive Theme Discovery Systems}, 
  author = {Alvin Po-Chun Chen and Dananjay Srinivas and Alexandra Barry and Maksim Seniw and Maria Leonor Pacheco},
  year = {2024},
  eprint = {2408.09030},
  archivePrefix = {arXiv},
  primaryClass = {cs.CL},
  html = {https://arxiv.org/abs/2408.09030},
  abstract = {NLP-assisted solutions have gained considerable traction to support qualitative data analysis. However, there does not exist a unified evaluation framework that can account for the many different settings in which qualitative researchers may employ them. In this paper, we take a first step in this direction by proposing an evaluation framework to study the way in which different tools may result in different outcomes depending on the collaboration strategy employed. Specifically, we study the impact of synchronous vs. asynchronous collaboration using two different NLP-assisted qualitative research tools and present a comprehensive analysis of significant differences in the consistency, cohesiveness, and correctness of their outputs. },
  selected = {true},
}
@inproceedings{chen-etal-2024-mothman,
  bibtex_show={true},
  title = {Mothman at {S}em{E}val-2024 Task 9: An Iterative System for Chain-of-Thought Prompt Optimization},
  author = {Chen, Alvin Po-Chun and Groshan, Ray and Von Bayern, Sean},
  editor = {Ojha, Atul Kr.  and
    Do{\u{g}}ru{\"o}z, A. Seza  and
    Tayyar Madabushi, Harish  and
    Da San Martino, Giovanni  and
    Rosenthal, Sara  and
    Ros{\'a}, Aiala},
  booktitle = {Proceedings of the 18th International Workshop on Semantic Evaluation (SemEval-2024)},
  month = jun,
  year = {2024},
  address = {Mexico City, Mexico},
  publisher = {Association for Computational Linguistics},
  url = {https://aclanthology.org/2024.semeval-1.263},
  html = {https://arxiv.org/abs/2405.02517},
  doi = {10.18653/v1/2024.semeval-1.263},
  pages = {1876--1888},
  abstract = {Extensive research exists on the performance of large language models on logic-based tasks, whereas relatively little has been done on their ability to generate creative solutions on lateral thinking tasks. The BrainTeaser shared task tests lateral thinking and uses adversarial datasets to prevent memorization, resulting in poor performance for out-of-the-box models. We propose a system for iterative, chain-of-thought prompt engineering which optimizes prompts using human evaluation. Using this shared task, we demonstrate our system{'}s ability to significantly improve model performance by optimizing prompts and evaluate the input dataset.},
  selected = {true}
}