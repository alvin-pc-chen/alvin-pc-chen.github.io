<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Scanning Machine Brains | Alvin Chen 陳柏駿 </title> <meta name="author" content="Alvin Chen 陳柏駿"> <meta name="description" content="Can supervised probing help us unlock the black box?"> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/favicon.ico?c6fddee0e29e79c86a7dd97fa38f204b"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://alvin-pc-chen.github.io/blog/2025/probing/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Alvin</span> Chen 陳柏駿 </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">repositories </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Scanning Machine Brains</h1> <p class="post-meta"> Created in July 15, 2025 </p> <p class="post-tags"> <a href="/blog/2025"> <i class="fa-solid fa-calendar fa-sm"></i> 2025 </a>   ·   <a href="/blog/tag/probing"> <i class="fa-solid fa-hashtag fa-sm"></i> probing,</a>   <a href="/blog/tag/bertology"> <i class="fa-solid fa-hashtag fa-sm"></i> bertology,</a>   <a href="/blog/tag/mechanistic"> <i class="fa-solid fa-hashtag fa-sm"></i> mechanistic</a>   <a href="/blog/tag/interpretability"> <i class="fa-solid fa-hashtag fa-sm"></i> interpretability,</a>   <a href="/blog/tag/graphspect"> <i class="fa-solid fa-hashtag fa-sm"></i> graphspect,</a>   <a href="/blog/tag/umr"> <i class="fa-solid fa-hashtag fa-sm"></i> umr,</a>   <a href="/blog/tag/compling"> <i class="fa-solid fa-hashtag fa-sm"></i> compling,</a>   <a href="/blog/tag/llms"> <i class="fa-solid fa-hashtag fa-sm"></i> LLMs,</a>   <a href="/blog/tag/aspect"> <i class="fa-solid fa-hashtag fa-sm"></i> aspect</a>   ·   <a href="/blog/category/research"> <i class="fa-solid fa-tag fa-sm"></i> research</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>I previously wrote about my UMR Aspect Annotation project, where we tried a variety of methods to use neural networks to label linguistic aspect. Surprisingly, even massive LLMs like GPT-4o and DeepSeek R1 struggle with aspect labeling through prompting alone. So—do they actually “understand” aspect? To investigate this, I ran supervised probes for every hidden layer of BERT across a selection of token positions using a target dataset.</p> <p><strong>Why this matters.</strong> Linguistics <em>should</em> be inherently interesting, but I know not everyone feels that way. Still, probing LLMs is a great lens for anyone trying to understand what these models actually do. As AI becomes more widespread, the need for interpretability increases-not just for academic curiosity, but to <a href="https://leonardbereska.github.io/blog/2024/mechinterpreview/#how-could-interpretability-promote-ai-safety" rel="external nofollow noopener" target="_blank">evaluate risks, bias, and trustworthiness</a>. Think of probing like putting an LLM into an MRI machine while it “thinks” about a concept. Which parts light up when it processes aspect? If we could identify these “neurons,” could we or <a href="https://transformer-circuits.pub/2024/scaling-monosemanticity/index.html#:~:text=For%20instance%2C%20we%20see%20that%20clamping%20the%20Golden%20Gate%20Bridge%20feature%2034M/31164353%20to%2010%C3%97%20its%20maximum%20activation%20value%20induces%20thematically%2Drelated%20model%20behavior" rel="external nofollow noopener" target="_blank">influence</a> the model’s behavior?</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/probing/intro-480.webp 480w,/assets/img/blog/probing/intro-800.webp 800w,/assets/img/blog/probing/intro-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/blog/probing/intro.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Sentences are passed through an LLM and embeddings are extracted from each hidden layer. Embeddings are grouped based on relative position to the first verb token and only ±10 positions are used. A probe is trained for each layer x position using the aspect label of the whole sentence. Code for this experiment can be found <a href="https://github.com/alvin-pc-chen/aspect_probe_release" rel="external nofollow noopener" target="_blank">here</a>; click <a href="#results">here</a> to skip to results and takeaways. </div> <h3 id="what-is-aspect-and-how-do-we-probe-for-it">What is aspect and how do we probe for it?</h3> <p>The GraphSpect framework we’re trying to build relies on LLM embeddings as the base layer in the architecture, based on the hypothesis that LLMs have a decent enough representation of linguistic phenomena. We’ve already shown that LLMs struggle with labeling aspect categories (in fact LLMs may have poor <a href="https://aclanthology.org/2024.dmr-1.12/" rel="external nofollow noopener" target="_blank">meta-linguistc knowledge</a> altogether), so if they don’t even capture aspect phenomena, then we’ll have to rethink our entire approach.</p> <p>For the less linguistically-inclined, aspect has to do with <strong>how</strong> an action or event unfolds over time. Unlike tense, which deals with <strong>when</strong> the event happens, aspect represents things like the duration or boundedness of the event. When we discuss the aspect of an event, we might ask questions such as “Is the event completed or ongoing?”, “Did it happen repeatedly?” or “Is it a single moment or a longer process?”</p> <p>English rarely marks aspect explicitly, but consider:</p> <ol> <li><em>I will have eaten by 8 o’clock.</em></li> <li><em>I will be eating by 8 o’clock.</em></li> </ol> <p>Although both events (the eating) take place in the future, we know that the first event has a fixed end point (is <strong>bounded</strong>) whereas the second does not. Another event could be <strong>habitual</strong>, for example in the sentence <em>“I eat at 8 o’clock”</em>. The meaning of this sentence is less clear without context, but one reading certainly is that the speaker regularly consumes food at this time of day. Enough background for now (before we get into <a href="https://plato.stanford.edu/entries/tense-aspect/#LexGraAsp" rel="external nofollow noopener" target="_blank">lexical vs. grammatical aspect</a>), let’s talk about probing.</p> <p><a href="https://www.youtube.com/watch?v=ElDtkhqv5ZE" rel="external nofollow noopener" target="_blank">Probing</a> is a fairly simple method for investigating the knowledge and behavior of neural networks. Supervised probes are nothing but classifiers that take some part of the LLM as input (in this case I’m looking at all the hidden layer outputs) to predict some phenomena as output (the aspect label). Continuing with the MRI metaphor, I’m scanning the LLM layer by layer, position by position, to identify where in the network aspect-related information might be encoded.</p> <p>There are a few downsides to using a simple probing technique here. Firstly probes are models themselves, which means that a powerful enough probe will learn the correct labels from what is essentially noise. While this isn’t an issue for my experiment—since my end goal is to train an aspect classifier—rigorous studies on model knowledge will need a variety of controls to ensure that the probes are actually identifying information in the LLM.</p> <p>Secondly, probing only demonstrates correlation, not causation. My classifiers can identify which layers and token positions produce embeddings that help predict aspect, but this doesn’t mean that LLMs inherently represent aspect in these places—or if they represent aspect at all. More powerful techniques exist, including the whole field of <a href="https://www.youtube.com/watch?v=veT2VI4vHyU" rel="external nofollow noopener" target="_blank">Mechanistic Interpretability</a>, which is more akin to doing brain surgery rather than simple scans. One of my inspirations for this probing experiment was <a href="https://aclanthology.org/2024.acl-long.785/" rel="external nofollow noopener" target="_blank">CausalGym</a>, where researchers identified a bunch of syntactic computations that models perform. I couldn’t figure out a way to apply this method to identify aspect computation, but if you have ideas definitely <a href="mailto:alvin.chen@colorado.edu">let me know</a>!</p> <h3 id="now-youre-thinking-with-probes">Now You’re Thinking with Probes</h3> <p>With all the background out of the way, let’s recap the investigation. We previously found that the LLM embeddings from the output layer aren’t good enough to predict aspect, so now we have to investigate whether or not LLMs capture good representations at all. We know from CausalGym and other research that LLMs process different parts of the sentence at different layers, so it’s possible that the intermediate layers are responsible for processing aspect information. To test this, we simply need to look at all the hidden layer outputs at all the embedding positions to find out if and where this is happening. Simple!</p> <p><strong>Problem 1: Natural language is diverse.</strong> Sentences can range from a few tokens to hundreds of tokens in length, contain no events or multiple interleaving events, and events themselves can comprise of a single or multiple tokens—or sometimes no tokens at all! As a practical workaround, I use the <a href="https://github.com/tttthomasssss/coling2020/" rel="external nofollow noopener" target="_blank">SitEnt dataset</a> which comprises mostly single-event sentences, and simply probe for ±10 token positions from the event verb. I also remove any edge cases where the sentence has multiple events or the event verb doesn’t show up in the surface form of the sentence. Regardless of the number of tokens in the verb event, I always take the center position to be the first token in the verb. The final dataset allows me to train probes using the aspect label for the single event verb of each sentence, closing the loop for the experiment.</p> <p><strong>Problem 2: Aspect is distributed.</strong> Unlike syntactic features, aspect doesn’t have regularized forms—at least in English. Where CausalGym can investigate with minimal pairs (e.g. <em>The authors are writing</em> vs. <em>The author is writing</em> to test for number agreement), there is no simple way to change a sentence to guarantee a change in aspect. If I say <em>I mowed the lawn</em>, we would assume that the lawn has been mowed and the event has completed with a result. However, if I say <em>I mowed the lawn for an hour</em>, it’s likely that the lawn hasn’t been fully mowed and no result state has been reached. There is no simple way to tweak LLMs to investigate their inherent understanding of aspect, which is why I resort to simply probing for a signal that a representation <em>could</em> exist.</p> <p><strong>Problem 3: Unidirectional LLMs store data differently.</strong> With the success of GPT models, most LLMs mask later tokens for training purposes. This means that the later embeddings “see” earlier tokens when doing self-attention, but not vice-versa. While this is useful for generative purposes, my downstream task is to model aspect information, which theoretically performs better when all embeddings “see” each other. On the other hand, larger models capture richer representations of language and are likely to better understand aspect. I’m limited to sampling bidirectional models to get useful results for my downstream task; the largest I could find was <a href="https://huggingface.co/answerdotai/ModernBERT-large" rel="external nofollow noopener" target="_blank">ModernBERT</a>. <br></p> <h3 id="implementation-details">Implementation Details</h3> <p>The experiment is fairly simple to execute, but there are a few details to note when replicating my methods. The full repository with instructions and documentation can be found <a href="https://github.com/alvin-pc-chen/aspect_probe_release" rel="external nofollow noopener" target="_blank">here</a>.</p> <p><strong>Making Experiment Data:</strong> The first task in this experiment is to extract hidden layer outputs from the selected LLM for all inputs and extract the correct token positions. I split this into two scripts, first extracting all outputs before selecting the embeddings that I want so that all embeddings are available in case I want to run additional experiments. As noted in problem 1, not all input sentences will have the same range of tokens, so I handle this by first searching through the tokens of each sentence to find valid positions within my desired range:</p> <details> <summary>Show Code</summary> <figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr>
<td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
</pre></td> <td class="code"><pre><span class="c1"># Load the data
</span>    <span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nf">read_csv</span><span class="p">(</span><span class="n">infile</span><span class="p">,</span> <span class="n">sep</span><span class="o">=</span><span class="sh">"</span><span class="se">\t</span><span class="sh">"</span><span class="p">)</span>
    <span class="n">df</span><span class="p">[</span><span class="sh">"</span><span class="s">label</span><span class="sh">"</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="sh">"</span><span class="s">label</span><span class="sh">"</span><span class="p">].</span><span class="nf">apply</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="mi">1</span> <span class="k">if</span> <span class="n">x</span> <span class="o">==</span> <span class="sh">"</span><span class="s">DYNAMIC</span><span class="sh">"</span> <span class="k">else</span> <span class="mi">0</span><span class="p">)</span>
    <span class="n">labels</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="sh">"</span><span class="s">label</span><span class="sh">"</span><span class="p">].</span><span class="nf">to_list</span><span class="p">()</span>
    <span class="n">texts</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="sh">"</span><span class="s">sentence</span><span class="sh">"</span><span class="p">].</span><span class="nf">to_list</span><span class="p">()</span>
    <span class="n">spans</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="sh">"</span><span class="s">start_char</span><span class="sh">"</span><span class="p">].</span><span class="nf">to_list</span><span class="p">()</span>

    <span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="p">.</span><span class="nf">from_pretrained</span><span class="p">(</span><span class="n">model_name</span><span class="p">)</span>

    <span class="c1"># For each input sentence, find valid token positions within range ±10
</span>    <span class="c1"># from the first token in the verb.
</span>    <span class="n">input_indices</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">texts</span><span class="p">)):</span>
        <span class="n">tokens</span> <span class="o">=</span> <span class="nf">tokenizer</span><span class="p">(</span><span class="n">texts</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">return_tensors</span><span class="o">=</span><span class="sh">"</span><span class="s">pt</span><span class="sh">"</span><span class="p">)</span>
        <span class="n">center</span> <span class="o">=</span> <span class="n">tokens</span><span class="p">.</span><span class="nf">char_to_token</span><span class="p">(</span><span class="n">spans</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
        <span class="n">indices</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">21</span><span class="p">):</span>
            <span class="n">idx</span> <span class="o">=</span> <span class="n">center</span> <span class="o">-</span> <span class="mi">10</span> <span class="o">+</span> <span class="n">j</span>
            <span class="k">if</span> <span class="n">idx</span> <span class="o">&lt;</span> <span class="mi">0</span> <span class="ow">or</span> <span class="n">idx</span> <span class="o">&gt;=</span> <span class="n">tokens</span><span class="p">.</span><span class="n">input_ids</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]:</span>
                <span class="k">continue</span>
            <span class="n">indices</span><span class="p">.</span><span class="nf">append</span><span class="p">((</span><span class="n">j</span><span class="p">,</span> <span class="n">idx</span><span class="p">))</span>
        <span class="n">input_indices</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">indices</span><span class="p">)</span>
    
</pre></td> </tr></tbody></table></code></pre></figure> </details> <p><br></p> <p>Once this is done I can simply pickle the tensors that I use for my experiment datasets. Generating embeddings this way allows me to both save memory and repeatedly use the same embeddings to check for any issues with later steps.</p> <p><strong>Custom Probes:</strong> It’s possible that aspect is represented non-linearly in BERT, which will not be found if we only investigate with linear classifiers. To keep my code modular, all probe objects should have the same input and output arguments as <code class="language-plaintext highlighter-rouge">torch.nn.Linear</code>:</p> <details> <summary>Show Code</summary> <figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr>
<td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
</pre></td> <td class="code"><pre><span class="k">class</span> <span class="nc">MultiLinearProbe</span><span class="p">(</span><span class="n">Module</span><span class="p">):</span>
        <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">in_features</span><span class="p">,</span> <span class="n">out_features</span><span class="p">):</span>
            <span class="nf">super</span><span class="p">(</span><span class="n">MultiLinearProbe</span><span class="p">,</span> <span class="n">self</span><span class="p">).</span><span class="nf">__init__</span><span class="p">()</span>
            <span class="n">self</span><span class="p">.</span><span class="n">linear1</span> <span class="o">=</span> <span class="nc">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="p">,</span> <span class="n">in_features</span> <span class="o">//</span> <span class="mi">2</span><span class="p">)</span>
            <span class="n">self</span><span class="p">.</span><span class="n">linear2</span> <span class="o">=</span> <span class="nc">Linear</span><span class="p">(</span><span class="n">in_features</span> <span class="o">//</span> <span class="mi">2</span><span class="p">,</span> <span class="n">out_features</span><span class="p">)</span>
            <span class="n">self</span><span class="p">.</span><span class="n">relu</span> <span class="o">=</span> <span class="nc">ReLU</span><span class="p">()</span>

        <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">relu</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="nf">linear1</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
            <span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="nf">linear2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    
</pre></td> </tr></tbody></table></code></pre></figure> </details> <p><br></p> <p><strong>Memory Efficient Probing:</strong> The biggest issue I encountered was getting exploding gradients leading to classifiers full of <code class="language-plaintext highlighter-rouge">nan</code> tensors. After a ton of debugging, the likely culprit seems to be loading all the datasets and classifiers into memory together. Whether I trained on GPUs or Apple Silicon, I would immediately get unreasonable losses when I expanded my search to looking at multiple hidden layers. I’m probing 21 positions per input sentence, and since BERT-large has 24 hidden layers (25 including the input layer), that means 25 x 21 = 525 classifiers training on 525 datasets! Fortunately, I’ve already saved my experiment data formatted ready for use, so I simply need to load each dataset sequentially and erase my cache after each experiment:</p> <details> <summary>Show Code</summary> <figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr>
<td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
</pre></td> <td class="code"><pre><span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">train_layers</span><span class="p">)):</span>
        <span class="n">train_position_paths</span> <span class="o">=</span> <span class="nf">glob</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="si">{</span><span class="n">train_layers</span><span class="p">[</span><span class="n">layer</span><span class="p">]</span><span class="si">}</span><span class="s">/position*.pkl</span><span class="sh">"</span><span class="p">)</span>
        <span class="n">train_position_paths</span><span class="p">.</span><span class="nf">sort</span><span class="p">(</span>
            <span class="n">key</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="nf">int</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="nf">split</span><span class="p">(</span><span class="sh">"</span><span class="s">/</span><span class="sh">"</span><span class="p">)[</span><span class="o">-</span><span class="mi">1</span><span class="p">].</span><span class="nf">split</span><span class="p">(</span><span class="sh">"</span><span class="s">position</span><span class="sh">"</span><span class="p">)[</span><span class="mi">1</span><span class="p">].</span><span class="nf">split</span><span class="p">(</span><span class="sh">"</span><span class="s">.pkl</span><span class="sh">"</span><span class="p">)[</span><span class="mi">0</span><span class="p">])</span>
        <span class="p">)</span>
        <span class="n">test_position_paths</span> <span class="o">=</span> <span class="nf">glob</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="si">{</span><span class="n">test_layers</span><span class="p">[</span><span class="n">layer</span><span class="p">]</span><span class="si">}</span><span class="s">/position*.pkl</span><span class="sh">"</span><span class="p">)</span>
        <span class="n">test_position_paths</span><span class="p">.</span><span class="nf">sort</span><span class="p">(</span>
            <span class="n">key</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="nf">int</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="nf">split</span><span class="p">(</span><span class="sh">"</span><span class="s">/</span><span class="sh">"</span><span class="p">)[</span><span class="o">-</span><span class="mi">1</span><span class="p">].</span><span class="nf">split</span><span class="p">(</span><span class="sh">"</span><span class="s">position</span><span class="sh">"</span><span class="p">)[</span><span class="mi">1</span><span class="p">].</span><span class="nf">split</span><span class="p">(</span><span class="sh">"</span><span class="s">.pkl</span><span class="sh">"</span><span class="p">)[</span><span class="mi">0</span><span class="p">])</span>
        <span class="p">)</span>
        <span class="k">for</span> <span class="n">position</span> <span class="ow">in</span> <span class="nf">tqdm</span><span class="p">(</span>
            <span class="nf">range</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">train_position_paths</span><span class="p">)),</span>
            <span class="n">desc</span><span class="o">=</span><span class="sa">f</span><span class="sh">"</span><span class="s">Layer </span><span class="si">{</span><span class="n">layer</span> <span class="o">+</span> <span class="mi">1</span><span class="si">}</span><span class="s">/</span><span class="si">{</span><span class="nf">len</span><span class="p">(</span><span class="n">train_layers</span><span class="p">)</span><span class="si">}</span><span class="sh">"</span><span class="p">,</span>
        <span class="p">):</span>
            <span class="n">labels</span><span class="p">,</span> <span class="n">tensors</span> <span class="o">=</span> <span class="n">pickle</span><span class="p">.</span><span class="nf">load</span><span class="p">(</span><span class="nf">open</span><span class="p">(</span><span class="n">train_position_paths</span><span class="p">[</span><span class="n">position</span><span class="p">],</span> <span class="sh">"</span><span class="s">rb</span><span class="sh">"</span><span class="p">))</span>
            <span class="n">ds</span> <span class="o">=</span> <span class="nc">CustomDataset</span><span class="p">(</span><span class="n">labels</span><span class="p">,</span> <span class="n">tensors</span><span class="p">,</span> <span class="n">tensors</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">shape</span><span class="p">.</span><span class="nf">numel</span><span class="p">())</span>
            <span class="n">train_dataloader</span> <span class="o">=</span> <span class="nc">DataLoader</span><span class="p">(</span>
                <span class="n">ds</span><span class="p">,</span>
                <span class="n">batch_size</span><span class="o">=</span><span class="n">train_batch_size</span><span class="p">,</span>
                <span class="n">shuffle</span><span class="o">=</span><span class="n">train_shuffle</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="n">classifier</span> <span class="o">=</span> <span class="nf">probe</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="n">ds</span><span class="p">.</span><span class="n">hs_dim</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="mi">2</span><span class="p">).</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
            <span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">optim</span><span class="p">.</span><span class="nc">SGD</span><span class="p">(</span>
                <span class="n">classifier</span><span class="p">.</span><span class="nf">parameters</span><span class="p">(),</span>
                <span class="n">lr</span><span class="o">=</span><span class="n">learn_rate</span><span class="p">,</span>
                <span class="n">momentum</span><span class="o">=</span><span class="n">momentum</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">epochs</span><span class="p">):</span>
                <span class="n">classifier</span><span class="p">.</span><span class="nf">train</span><span class="p">(</span><span class="n">mode</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
                <span class="n">loss</span> <span class="o">=</span> <span class="nf">_train_one_epoch</span><span class="p">(</span><span class="n">classifier</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">train_dataloader</span><span class="p">)</span>
            <span class="n">test_labels</span><span class="p">,</span> <span class="n">test_tensors</span> <span class="o">=</span> <span class="n">pickle</span><span class="p">.</span><span class="nf">load</span><span class="p">(</span>
                <span class="nf">open</span><span class="p">(</span><span class="n">test_position_paths</span><span class="p">[</span><span class="n">position</span><span class="p">],</span> <span class="sh">"</span><span class="s">rb</span><span class="sh">"</span><span class="p">)</span>
            <span class="p">)</span>
            <span class="n">test_ds</span> <span class="o">=</span> <span class="nc">CustomDataset</span><span class="p">(</span>
                <span class="n">test_labels</span><span class="p">,</span> <span class="n">test_tensors</span><span class="p">,</span> <span class="n">test_tensors</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">shape</span><span class="p">.</span><span class="nf">numel</span><span class="p">()</span>
            <span class="p">)</span>
            <span class="n">test_dataloader</span> <span class="o">=</span> <span class="nc">DataLoader</span><span class="p">(</span>
                <span class="n">test_ds</span><span class="p">,</span>
                <span class="n">batch_size</span><span class="o">=</span><span class="n">test_batch_size</span><span class="p">,</span>
                <span class="n">shuffle</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="n">true</span><span class="p">,</span> <span class="n">pred</span> <span class="o">=</span> <span class="nf">_eval_one_epoch</span><span class="p">(</span><span class="n">classifier</span><span class="p">,</span> <span class="n">test_dataloader</span><span class="p">)</span>
            <span class="n">acc</span> <span class="o">=</span> <span class="n">metrics</span><span class="p">.</span><span class="nf">accuracy_score</span><span class="p">(</span><span class="n">true</span><span class="p">,</span> <span class="n">pred</span><span class="p">)</span>
            <span class="n">macro_f1</span> <span class="o">=</span> <span class="n">metrics</span><span class="p">.</span><span class="nf">f1_score</span><span class="p">(</span><span class="n">true</span><span class="p">,</span> <span class="n">pred</span><span class="p">,</span> <span class="n">average</span><span class="o">=</span><span class="sh">"</span><span class="s">macro</span><span class="sh">"</span><span class="p">)</span>
            <span class="n">micro_f1</span> <span class="o">=</span> <span class="n">metrics</span><span class="p">.</span><span class="nf">f1_score</span><span class="p">(</span><span class="n">true</span><span class="p">,</span> <span class="n">pred</span><span class="p">,</span> <span class="n">average</span><span class="o">=</span><span class="sh">"</span><span class="s">micro</span><span class="sh">"</span><span class="p">)</span>
            <span class="n">results</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span>
                <span class="p">{</span>
                    <span class="sh">"</span><span class="s">hidden_layer</span><span class="sh">"</span><span class="p">:</span> <span class="n">layer</span><span class="p">,</span>
                    <span class="sh">"</span><span class="s">token_position</span><span class="sh">"</span><span class="p">:</span> <span class="n">position</span><span class="p">,</span>
                    <span class="sh">"</span><span class="s">accuracy</span><span class="sh">"</span><span class="p">:</span> <span class="n">acc</span><span class="p">,</span>
                    <span class="sh">"</span><span class="s">macro_f1</span><span class="sh">"</span><span class="p">:</span> <span class="n">macro_f1</span><span class="p">,</span>
                    <span class="sh">"</span><span class="s">micro_f1</span><span class="sh">"</span><span class="p">:</span> <span class="n">micro_f1</span><span class="p">,</span>
                <span class="p">}</span>
            <span class="p">)</span>
            <span class="k">if</span> <span class="n">device</span> <span class="o">==</span> <span class="sh">"</span><span class="s">mps</span><span class="sh">"</span><span class="p">:</span>
                <span class="n">torch</span><span class="p">.</span><span class="n">mps</span><span class="p">.</span><span class="nf">empty_cache</span><span class="p">()</span>
            <span class="k">elif</span> <span class="n">device</span> <span class="o">==</span> <span class="sh">"</span><span class="s">cuda</span><span class="sh">"</span><span class="p">:</span>
                <span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="nf">empty_cache</span><span class="p">()</span>
    
</pre></td> </tr></tbody></table></code></pre></figure> </details> <p><br></p> <h3 id="results">Results</h3> <p>I tested <a href="https://huggingface.co/google-bert/bert-large-uncased" rel="external nofollow noopener" target="_blank">BERT-large</a> and <a href="https://huggingface.co/answerdotai/ModernBERT-large" rel="external nofollow noopener" target="_blank">ModernBERT</a> on the SitEnt dataset, using both linear and non-linear probes (see <a href="#implementation-details">above</a>). The cleaned dataset is comprised of 3,967 train samples and 938 test samples with two aspect labels, Stative and Dynamic. Importantly, probes at different token positions will see different percentages of the dataset due to the variation in sentence length. Although this potentially biases the experiment, in practice this discrepancy doesn’t impact the overall findings:</p> <details> <summary>Show Sample Distribution</summary> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/probing/sample_graph-480.webp 480w,/assets/img/blog/probing/sample_graph-800.webp 800w,/assets/img/blog/probing/sample_graph-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/blog/probing/sample_graph.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> The full dataset is present for token positions -2 to 2, and most of the dataset is available for positions -5 to 6. </div> </details> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/probing/bert_linear_16-480.webp 480w,/assets/img/blog/probing/bert_linear_16-800.webp 800w,/assets/img/blog/probing/bert_linear_16-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/blog/probing/bert_linear_16.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/probing/bert_linear_32-480.webp 480w,/assets/img/blog/probing/bert_linear_32-800.webp 800w,/assets/img/blog/probing/bert_linear_32-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/blog/probing/bert_linear_32.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/probing/bert_multi_16-480.webp 480w,/assets/img/blog/probing/bert_multi_16-800.webp 800w,/assets/img/blog/probing/bert_multi_16-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/blog/probing/bert_multi_16.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/probing/bert_multi_32-480.webp 480w,/assets/img/blog/probing/bert_multi_32-800.webp 800w,/assets/img/blog/probing/bert_multi_32-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/blog/probing/bert_multi_32.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Results from probing BERT-large with learning rate of 0.01 and batch size of 16. The first two use linear probes and bottom two use non-linear probes. Left results are trained for 16 epochs while the right results are trained for 32. </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/probing/mbert_linear_16-480.webp 480w,/assets/img/blog/probing/mbert_linear_16-800.webp 800w,/assets/img/blog/probing/mbert_linear_16-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/blog/probing/mbert_linear_16.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/probing/mbert_linear_32-480.webp 480w,/assets/img/blog/probing/mbert_linear_32-800.webp 800w,/assets/img/blog/probing/mbert_linear_32-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/blog/probing/mbert_linear_32.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/probing/mbert_multi_16-480.webp 480w,/assets/img/blog/probing/mbert_multi_16-800.webp 800w,/assets/img/blog/probing/mbert_multi_16-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/blog/probing/mbert_multi_16.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/probing/mbert_multi_32-480.webp 480w,/assets/img/blog/probing/mbert_multi_32-800.webp 800w,/assets/img/blog/probing/mbert_multi_32-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/blog/probing/mbert_multi_32.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Results from probing ModernBERT with learning rate of 0.001 and batch size of 8. The first two use linear probes and bottom two use non-linear probes. Left results are trained for 16 epochs while the right results are trained for 32. </div> <h3 id="findings--future-experiments">Findings &amp; Future Experiments</h3> <p>The results clearly demonstrate that certain token positions produce better aspect embeddings, even when the full dataset is available (from -2 to 2). No individual probe performs particularly well, but this is to be expected since each layer x position only represents a piece of the semantics. Across the board, the linear probes show the starkest contrast in performance for both token positions and hidden layers, suggesting that aspect is encoded linearly. The ModernBERT results contain regions of identically poor accuracies, but this is due to exploding gradients breaking probe weights. Two findings in particular are surprising and warrant further investigation:</p> <p><strong>Finding 1: Aspect is strongly correlated with the token before the verb.</strong> Although we expect the verb token to capture the best representation, the embeddings in the -1 position also performs quite well. In fact, the -1 position often performs better than the +1 position, even though the +1 position is sometimes part of the verb! This suggests that the model stores a part of the semantics in neighboring positions and needs to be accounted for when using individual embeddings. The fact that the phenomenon is more pronounced at higher epochs strengthens this finding and warrants further study. When looking at other LLM phenomena, it’s useful to investigate <em>fuzzily</em> by looking at neighboring or tangentially related embeddings where models may unintuitively encode information.</p> <p><strong>Finding 2: Intermediate hidden layers seem to capture better representations.</strong> Moreover, probes trained on outputs at the input layer (layer 0) and the output layer show less variance across token positions. This finding strengthens my hypothesis that aspect semantics is being processed within the model but becomes less important for the output layer, and explains why results from my previous exploration performed poorly. Confoundingly, which layer produces the best representation varies across experiments (including ones not shown here). Without further investigation, it’s difficult to select which layer output to use for downstream tasks. Interestingly, for a few of the experiments the probe at the -1 position in the input layer also produces strong results, sometimes better than the probe at the 0 position in the output layer. One possibility is that the -1 token is strongly correlated with a part of speech that impacts the meaning of the verb.</p> <p><strong>Next Steps: <em>Fuzzily</em> probe models by semantic roles.</strong> Based on these two findings, I can verify that LLMs capture useful representations for predicting aspect located near the verb token. However, only the verb token position is linked to a part of speech while other positions are somewhat arbitrary. The next investigation should instead group hidden layer outputs based on semantic roles, keeping in mind that neighboring positions might also encode some of the desired information.I would also be able to test sentences with multiple event verbs, since parsers generally assign roles to a target verb. The results of this experiment could paint a more complete picture of aspect representation in LLMs, especially if different layers produce better probes for each semantic role. <br></p> <h3 id="takeaways-for-your-probing-experiments">Takeaways for Your Probing Experiments</h3> <blockquote> <ol> <li>Hypothesize about model behavior by thinking about how humans conceptualize information to focus your lens. If I didn’t think about verb positions, my data would not look clear at all.</li> <li>Find the easiest experiment to run and investigate iteratively. Verifying my hypotheses about intermediate layers and token positions ensures that helped me find the next place to look.</li> <li>Run multiple versions of the same experiment: probing is an art, not a science. I wouldn’t have figured out my gradient issue if I just had one heatmap to look at and probably would have concluded that BERT doesn’t capture aspect.</li> <li>Sometimes features are non-linear, so it’s always smart to test with different types of probes. However, if your probes get too complex you should benchmark with randomized data to ensure that your findings are real.</li> </ol> </blockquote> </div> </article> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Alvin Chen 陳柏駿. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Last updated: August 06, 2025. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.min.js" integrity="sha256-rjmgmaB99riUNcdlrDtcAiwtLIojSxNyUFdl+Qh+rB4=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> <script type="module" src="/assets/js/search/ninja-keys.min.js?601a2d3465e2a52bec38b600518d5f70"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script>let searchTheme=determineComputedTheme();const ninjaKeys=document.querySelector("ninja-keys");"dark"===searchTheme?ninjaKeys.classList.add("dark"):ninjaKeys.classList.remove("dark");const openSearchModal=()=>{const e=$("#navbarNav");e.hasClass("show")&&e.collapse("hide"),ninjaKeys.open()};</script> <script>const ninja=document.querySelector("ninja-keys");ninja.data=[{id:"nav-about",title:"about",section:"Navigation",handler:()=>{window.location.href="/"}},{id:"nav-blog",title:"blog",description:"",section:"Navigation",handler:()=>{window.location.href="/blog/"}},{id:"nav-publications",title:"publications",description:"",section:"Navigation",handler:()=>{window.location.href="/publications/"}},{id:"nav-projects",title:"projects",description:"I update my projects once in a while.",section:"Navigation",handler:()=>{window.location.href="/projects/"}},{id:"nav-repositories",title:"repositories",description:"For detailed descriptions please view projects!",section:"Navigation",handler:()=>{window.location.href="/repositories/"}},{id:"nav-cv",title:"cv",description:"",section:"Navigation",handler:()=>{window.location.href="/cv/"}},{id:"post-scanning-machine-brains",title:"Scanning Machine Brains",description:"Can supervised probing help us unlock the black box?",section:"Posts",handler:()=>{window.location.href="/blog/2025/probing/"}},{id:"news-the-umr-project-released-the-2-0-dataset-and-the-english-corpus-now-has-over-30k-sentences-https-lindat-mff-cuni-cz-repository-items-239427de-bcaa-401d-a0ae-2c69602daa67",title:"[The UMR Project released the 2.0 dataset and the English corpus now has over 30k sentences!](https://lindat.mff.cuni.cz/repository/items/239427de-bcaa-401d-a0ae-2c69602daa67)",description:"",section:"News"},{id:"news-i-built-a-convolutional-neural-network-in-numpy-that-performs-at-tensorflow-levels-projects-cnn",title:"[I built a Convolutional Neural Network in NumPy that performs at TensorFlow levels!](/projects/cnn/)",description:"",section:"News"},{id:"news-i-implemented-an-irony-detector-for-tweets-that-outperforms-sota-projects-irony",title:"[I implemented an Irony Detector for tweets that outperforms SOTA!](/projects/irony/)",description:"",section:"News"},{id:"projects-numpy-cnn",title:"numpy cnn",description:"A Convolutional Neural Network implemented in NumPy with modular layers.",section:"Projects",handler:()=>{window.location.href="/projects/cnn/"}},{id:"projects-irony-detector",title:"irony detector",description:"A BiLSTM that detects irony in tweets based off SemEval 2018 Task 3.",section:"Projects",handler:()=>{window.location.href="/projects/irony/"}},{id:"socials-email",title:"Send email",section:"Socials",handler:()=>{window.open("mailto:%61%6C%76%69%6E.%63%68%65%6E@%63%6F%6C%6F%72%61%64%6F.%65%64%75","_blank")}},{id:"socials-github",title:"GitHub",section:"Socials",handler:()=>{window.open("https://github.com/alvin-pc-chen","_blank")}},{id:"socials-linkedin",title:"LinkedIn",section:"Socials",handler:()=>{window.open("https://www.linkedin.com/in/alvin-pc-chen","_blank")}},{id:"socials-medium",title:"Medium",section:"Socials",handler:()=>{window.open("https://medium.com/@alch6097","_blank")}},{id:"socials-bluesky",title:"Bluesky",section:"Socials",handler:()=>{window.open("https://bsky.app/profile/https://bsky.app/profile/alvinchen.bsky.social","_blank")}},{id:"light-theme",title:"Change theme to light",description:"Change the theme of the site to Light",section:"Theme",handler:()=>{setThemeSetting("light")}},{id:"dark-theme",title:"Change theme to dark",description:"Change the theme of the site to Dark",section:"Theme",handler:()=>{setThemeSetting("dark")}},{id:"system-theme",title:"Use system default theme",description:"Change the theme of the site to System Default",section:"Theme",handler:()=>{setThemeSetting("system")}}];</script> <script src="/assets/js/shortcut-key.js"></script> </body> </html>