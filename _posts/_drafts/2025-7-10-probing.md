---
layout: post
title: probing
date: 2025-07-15 21:01:00
description: placeholder
tags: probing, bertology, mechanistic interpretability, graphspect, umr, compling, LLMs, aspect
categories: research
thumbnail: assets/img/blog/ai-coding/profile.png
---

I previously wrote about my UMR Aspect Annotation project, where we tried a variety of methods to use neural networks to label linguistic aspect. Surprisingly, we found that even commercial LLMs such as GPT-4o and DeepSeek R1 seemed to struggle with this task using prompting alone, which begs the question: do LLMs even understand what aspect is? To investigate this, I used the classic ["BERTology"](https://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00349/96482/A-Primer-in-BERTology-What-We-Know-About-How-BERT) technique where I simply run supervised probes for every hidden layer of an LLM across a selection of token positions for each sentence. At least in theory, these probes should help me identify which layers or token positions capture the best representations of aspect.

#### Why Probe LLMs for Aspect?
The GraphSpect framework we're trying to build relies on LLM embeddings as the base layer in the architecture, based on the hypothesis that LLMs have a decent enough representation of linguistic phenomena. We've already shown that LLMs struggle with labeling aspect categories (in fact LLMs may have poor [meta-linguistc knowledge](https://aclanthology.org/2024.dmr-1.12/) altogether), so if they don't even capture aspect phenomena, then we'll have to rethink our entire approach.

For the less linguistically-inclined, aspect has to do with **how** an action or event unfolds over time. Unlike tense, which deals with **when** the event happens, aspect represents things like the duration or boundedness of the event. When we discuss the aspect of an event, we might ask questions such as:
- Is the event completed or ongoing?
- Did it happen repeatedly?
- Is it a single moment or a longer process?

While English doesn't explicitly mark aspect (most of the time), consider the following examples:
1. I will have eaten by 8 o'clock.
2. I will be eating by 8 o'clock.

Although both events (the eating) take place in the future, we know that the first event has a fixed end point (is **bounded**) whereas the second does not. Another event could be **habitual**, for example in the sentence _"I eat at 8 o'clock"_. The meaning of this sentence is less clear without context, but one reading certainly is that the speaker regularly consumes food at this time of day. Enough background for now (before we get into [lexical vs. grammatical aspect](https://plato.stanford.edu/entries/tense-aspect/#LexGraAsp)), let's talk about probing.

[Probing](https://www.youtube.com/watch?v=ElDtkhqv5ZE) is a fairly simple method for investigating the knowledge and behavior of neural networks. Supervised probes are nothing but classifiers that take some part of the LLM as input (in this case I'm looking at all the hidden layer outputs) to predict some phenomena as output (the aspect label). If we anthropomorphize models for a second, we can think of this probing process like putting the LLM under an MRI while we ask it to read sentences to determine which parts of its brain lights up. I use this technique in order to  

Essentially, all I'm doing is using every hidden layer output from the model as an input to a simple linear model


(((To use brain scans as an analogy, we can think about probes as ))). [Mech Interp](https://www.youtube.com/watch?v=veT2VI4vHyU)

#### Thinking with Probes



#### Implementing Probes

#### Findings

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid path="assets/img/blog/probing/linear_16_epochs.png" class="img-fluid rounded z-depth-1" zoomable=true %}
    </div>
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid path="assets/img/blog/probing/linear_32_epochs.png" class="img-fluid rounded z-depth-1" zoomable=true %}
    </div>
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid path="assets/img/blog/probing/linear_32_epochs_2.png" class="img-fluid rounded z-depth-1" zoomable=true %}
    </div>
</div>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid path="assets/img/blog/probing/multi_16_epochs.png" class="img-fluid rounded z-depth-1" zoomable=true %}
    </div>
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid path="assets/img/blog/probing/multi_32_epochs.png" class="img-fluid rounded z-depth-1" zoomable=true %}
    </div>
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid path="assets/img/blog/probing/multi_32_epochs_2.png" class="img-fluid rounded z-depth-1" zoomable=true %}
    </div>
</div>