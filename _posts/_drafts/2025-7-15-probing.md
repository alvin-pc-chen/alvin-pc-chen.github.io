---
layout: post
title: Scanning Machine Brains-Can Probing Open the Black Box?
date: 2025-07-10 21:01:00
description: Do LLMs even understand aspect? Is aspect even real? 
tags: probing, bertology, mechanistic interpretability, graphspect, umr, compling, LLMs, aspect
categories: research
thumbnail: assets/img/blog/probing/profile.jpg
---

I previously wrote about my UMR Aspect Annotation project, where we tried a variety of methods to use neural networks to label linguistic aspect. Surprisingly, even huge commercial LLMs like GPT-4o and DeepSeek R1 struggle with aspect labeling through prompting alone. So—do they actually "understand" aspect? To investigate this, I ran supervised probes for every hidden layer of BERT across a selection of token positions using a target dataset. 

**Why this matters.** Linguistics _should_ be inherently interesting, but I know not everyone feels that way. Still, probing LLMs is a great lens for anyone trying to understand what these models actually do. As AI becomes more widespread, the need for interpretability increases-not just for academic curiosity, but to [evaluate risks, bias, and trustworthiness](https://leonardbereska.github.io/blog/2024/mechinterpreview/#how-could-interpretability-promote-ai-safety). 

Think of probing like putting an LLM in an MRI while it responds to questions. Which parts light up when it “thinks” about a concept? If we identified the corresponding parameters, could we [shift](https://transformer-circuits.pub/2024/scaling-monosemanticity/index.html#:~:text=For%20instance%2C%20we%20see%20that%20clamping%20the%20Golden%20Gate%20Bridge%20feature%2034M/31164353%20to%2010%C3%97%20its%20maximum%20activation%20value%20induces%20thematically%2Drelated%20model%20behavior) how the model "acts"?

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/blog/probing/intro.png" class="img-fluid rounded z-depth-1" %}
    </div>
</div>
<div class="caption">
    What happens when probing goes wrong. Code for this experiment can be found <a href="https://github.com/alvin-pc-chen/aspect_probe_release">here</a>; click <a href="#findings">here</a> if you want to skip to the results.
</div>

_Code for this experiment can be found [here](https://github.com/alvin-pc-chen/aspect_probe_release); click [here](#results--findings) if you want to skip to the results._


What happens when probing goes wrong. Code for this experiment can be found [here](https://github.com/alvin-pc-chen/aspect_probe_release), and click [here](#results--findings) if you want to skip to the results.

### What is aspect and how do we probe for it?
The GraphSpect framework we're trying to build relies on LLM embeddings as the base layer in the architecture, based on the hypothesis that LLMs have a decent enough representation of linguistic phenomena. We've already shown that LLMs struggle with labeling aspect categories (in fact LLMs may have poor [meta-linguistc knowledge](https://aclanthology.org/2024.dmr-1.12/) altogether), so if they don't even capture aspect phenomena, then we'll have to rethink our entire approach.

For the less linguistically-inclined, aspect has to do with **how** an action or event unfolds over time. Unlike tense, which deals with **when** the event happens, aspect represents things like the duration or boundedness of the event. When we discuss the aspect of an event, we might ask questions such as "Is the event completed or ongoing?", "Did it happen repeatedly?" or "Is it a single moment or a longer process?"

English rarely marks aspect explicitly, but consider:
1. _I will have eaten by 8 o'clock._
2. _I will be eating by 8 o'clock._

Although both events (the eating) take place in the future, we know that the first event has a fixed end point (is **bounded**) whereas the second does not. Another event could be **habitual**, for example in the sentence _"I eat at 8 o'clock"_. The meaning of this sentence is less clear without context, but one reading certainly is that the speaker regularly consumes food at this time of day. Enough background for now (before we get into [lexical vs. grammatical aspect](https://plato.stanford.edu/entries/tense-aspect/#LexGraAsp)), let's talk about probing.

[Probing](https://www.youtube.com/watch?v=ElDtkhqv5ZE) is a fairly simple method for investigating the knowledge and behavior of neural networks. Supervised probes are nothing but classifiers that take some part of the LLM as input (in this case I'm looking at all the hidden layer outputs) to predict some phenomena as output (the aspect label). Continuing with the MRI metaphor, I'm scanning the LLM as it reads sentences to figure out which "thoughts" correspond to the aspect of events.

There are a few downsides to using a simple probing technique here. Firstly probes (especially supervised probes) are models themselves, which means that a powerful enough probe will learn the correct labels from what is essentially noise. While this isn't an issue for my experiment---since my end goal is to train an aspect classifier---rigorous studies on model knowledge will need a variety of controls to ensure that the probes are actually identifying information in the LLM.

A related problem is that probing can only demonstrate correlation but not causation. My classifiers can identify which layers and token positions produce embeddings that help predict aspect, but this doesn't mean that LLMs inherently represent aspect in these places---or if they represent aspect at all. More powerful techniques exist, including the whole field of [Mechanistic Interpretability](https://www.youtube.com/watch?v=veT2VI4vHyU), which is more akin to doing brain surgery rather than simple scans. One of my inspirations for this probing experiment was [CausalGym](https://aclanthology.org/2024.acl-long.785/), where researchers identified a bunch of syntactic computations that models perform. I couldn't figure out a way to apply this method to identify aspect computation, but if you have ideas definitely [let me know](mailto:alvin.chen@colorado.edu)!

### Now You're Thinking with Probes
With all the background out of the way, let's recap the investigation. We previously found that the LLM embeddings from the output layer aren't good enough to predict aspect, so now we have to investigate whether or not LLMs capture good representations at all. We know from CausalGym and other research that LLMs process different parts of the sentence at different layers, so it's possible that the intermediate layers are responsible for processing aspect information. To test this, we simply need to look at all the hidden layer outputs at all the embedding positions to find out if and where this is happening. Simple!

**Problem 1: Natural language is diverse.**
Sentences can range from a few tokens to hundreds of tokens in length, contain no events or multiple interleaving events, and events themselves can comprise of a single or multiple tokens---or sometimes no tokens at all! As a practical workaround, I use the [SitEnt dataset](https://github.com/tttthomasssss/coling2020/) which comprises mostly single-event sentences, and simply probe for ±10 token positions from the event verb. I also remove any edge cases where the sentence has multiple events or the event verb doesn't show up in the surface form of the sentence. Regardless of the number of tokens in the verb event, I always take the center position to be the first token in the verb. The final dataset allows me to train probes using the aspect label for the single event verb of each sentence, closing the loop for the experiment.

**Problem 2: Aspect is distributed.**
Unlike syntactic features, aspect doesn't have regularized forms---at least in English. Where CausalGym can investigate with minimal pairs (e.g. _The authors are writing_ vs. _The author is writing_ to test for number agreement), there is no simple way to change a sentence to guarantee a change in aspect. If I say _I mowed the lawn_, we would assume that the lawn has been mowed and the event has completed with a result. However, if I say _I mowed the lawn for an hour_, it's likely that the lawn hasn't been fully mowed and no result state has been reached. There is no simple way to tweak LLMs to investigate their inherent understanding of aspect, which is why I resort to simply probing for a signal that a representation _could_ exist.

**Problem 3: Unidirectional LLMs store data differently.**
With the success of GPT models, most LLMs mask later tokens for training purposes. This means that the later embeddings "see" earlier tokens when doing self-attention, but not vice-versa. While this is useful for generative purposes, my downstream task is to model aspect information, which theoretically performs better when all embeddings "see" each other. On the other hand, larger models capture richer representations of language and are likely to better understand aspect. I'm limited to sampling bidirectional models to get useful results for my downstream task; the largest I could find was [ModernBERT](https://huggingface.co/answerdotai/ModernBERT-large).
<br>

### Implementation Details
The experiment is fairly simple to execute, but there are a few details to note when replicating my methods. The full repository with instructions and documentation can be found [here](https://github.com/alvin-pc-chen/aspect_probe_release).

**Making Experiment Data:**
The first task in this experiment is to extract hidden layer outputs from the selected LLM for all inputs and extract the correct token positions. I split this into two scripts, first extracting all outputs before selecting the embeddings that I want so that all embeddings are available in case I want to run additional experiments. As noted in problem 1, not all input sentences will have the same range of tokens, so I handle this by first searching through the tokens of each sentence to find valid positions within my desired range:

<details>
    <summary>Show Code</summary>
    {% highlight python linenos %}# Load the data
    df = pd.read_csv(infile, sep="\t")
    df["label"] = df["label"].apply(lambda x: 1 if x == "DYNAMIC" else 0)
    labels = df["label"].to_list()
    texts = df["sentence"].to_list()
    spans = df["start_char"].to_list()

    tokenizer = AutoTokenizer.from_pretrained(model_name)

    # For each input sentence, find valid token positions within range ±10
    # from the first token in the verb.
    input_indices = []
    for i in range(len(texts)):
        tokens = tokenizer(texts[i], return_tensors="pt")
        center = tokens.char_to_token(spans[i])
        indices = []
        for j in range(21):
            idx = center - 10 + j
            if idx < 0 or idx >= tokens.input_ids.shape[1]:
                continue
            indices.append((j, idx))
        input_indices.append(indices)
    {% endhighlight %}
</details>
<br>

Once this is done I can simply pickle the tensors that I use for my experiment datasets. Generating embeddings this way allows me to both save memory and repeatedly use the same embeddings to check for any issues with later steps.

**Custom Probes:**
It's possible that aspect is represented non-linearly in BERT, which will not be found if we only investigate with linear classifiers. To keep my code modular, all probe objects should have the same input and output arguments as `torch.nn.Linear`:

<details>
    <summary>Show Code</summary>
    {% highlight python linenos %}class MultiLinearProbe(Module):
        def __init__(self, in_features, out_features):
            super(MultiLinearProbe, self).__init__()
            self.linear1 = Linear(in_features, in_features // 2)
            self.linear2 = Linear(in_features // 2, out_features)
            self.relu = ReLU()

        def forward(self, x):
            x = self.relu(self.linear1(x))
            return self.linear2(x)
    {% endhighlight %}
</details>
<br>

**Memory Efficient Probing:**
The biggest issue I encountered was getting exploding gradients leading to classifiers full of `nan` tensors. After a ton of debugging, the likely culprit seems to be loading all the datasets and classifiers into memory together. Whether I trained on GPUs or Apple Silicon, I would immediately get unreasonable losses when I expanded my search to looking at multiple hidden layers. I'm probing 21 positions per input, and since BERT-large has 24 hidden layers (25 including the embedding layer), that means 25 x 21 = 525 classifiers training on 525 datasets! Fortunately, I've already saved my experiment data formatted ready for use, so I simply need to load each dataset sequentially and erase my cache after each experiment:

<details>
    <summary>Show Code</summary>
    {% highlight python linenos %}for layer in range(len(train_layers)):
        train_position_paths = glob(f"{train_layers[layer]}/position*.pkl")
        train_position_paths.sort(
            key=lambda x: int(x.split("/")[-1].split("position")[1].split(".pkl")[0])
        )
        test_position_paths = glob(f"{test_layers[layer]}/position*.pkl")
        test_position_paths.sort(
            key=lambda x: int(x.split("/")[-1].split("position")[1].split(".pkl")[0])
        )
        for position in tqdm(
            range(len(train_position_paths)),
            desc=f"Layer {layer + 1}/{len(train_layers)}",
        ):
            labels, tensors = pickle.load(open(train_position_paths[position], "rb"))
            ds = CustomDataset(labels, tensors, tensors[0].shape.numel())
            train_dataloader = DataLoader(
                ds,
                batch_size=train_batch_size,
                shuffle=train_shuffle,
            )
            classifier = probe(in_features=ds.hs_dim, out_features=2).to(device)
            optimizer = torch.optim.SGD(
                classifier.parameters(),
                lr=learn_rate,
                momentum=momentum,
            )
            for epoch in range(epochs):
                classifier.train(mode=True)
                loss = _train_one_epoch(classifier, optimizer, train_dataloader)
            test_labels, test_tensors = pickle.load(
                open(test_position_paths[position], "rb")
            )
            test_ds = CustomDataset(
                test_labels, test_tensors, test_tensors[0].shape.numel()
            )
            test_dataloader = DataLoader(
                test_ds,
                batch_size=test_batch_size,
                shuffle=False,
            )
            true, pred = _eval_one_epoch(classifier, test_dataloader)
            acc = metrics.accuracy_score(true, pred)
            macro_f1 = metrics.f1_score(true, pred, average="macro")
            micro_f1 = metrics.f1_score(true, pred, average="micro")
            results.append(
                {
                    "hidden_layer": layer,
                    "token_position": position,
                    "accuracy": acc,
                    "macro_f1": macro_f1,
                    "micro_f1": micro_f1,
                }
            )
            if device == "mps":
                torch.mps.empty_cache()
            elif device == "cuda":
                torch.cuda.empty_cache()
    {% endhighlight %}
</details>
<br>

### Results & Findings
In summary, I tested both [BERT-large](https://huggingface.co/google-bert/bert-large-uncased) and [ModernBERT](https://huggingface.co/answerdotai/ModernBERT-large) using the SitEnt dataset, which has only two possible aspect labels (Stative and Dynamic). 

An important note about the data is the discrepancy in the number of sample for each position due to the differing length of sentences. We're guaranteed to have the full dataset (~4k train sentences, ~1k test) to train the probes in the center, but as we go towards either end there are fewer sentences with the proper length (about half as many at the extremes). For example, the sentence _Understand that a job worth doing is worth doing crappily._ with the verb "is" will have 6 tokens before the verb and 3 tokens after, meaning that the probes for the first 4 and final 7 positions will not see this input. The upshot is that results should be skewed towards 

<details>
    <summary>Show Data Distribution</summary>
    <table id="table" data-toggle="table" class="text-center">
        <thead>
            <tr>
                <th rowspan=2>Model</th>
                <th rowspan=2>Dataset</th>
                <th class=center colspan=21>Token Position</th>
            </tr>
            <tr>
                <th>-10</th>
                <th>-9</th>
                <th>-8</th>
                <th>-7</th>
                <th>-6</th>
                <th>-5</th>
                <th>-4</th>
                <th>-3</th>
                <th>-2</th>
                <th>-1</th>
                <th>0</th>
                <th>1</th>
                <th>2</th>
                <th>3</th>
                <th>4</th>
                <th>5</th>
                <th>6</th>
                <th>7</th>
                <th>8</th>
                <th>9</th>
                <th>10</th>
            </tr>
        </thead>
        <tbody>
            <tr>
                <th rowspan=2>BERT-large</th>
                <th>train</th>
                <th>1924</th>
                <th>2105</th>
                <th>2297</th>
                <th>2482</th>
                <th>2691</th>
                <th>2964</th>
                <th>3237</th>
                <th>3593</th>
                <th>3967</th>
                <th>3967</th>
                <th>3967</th>
                <th>3967</th>
                <th>3965</th>
                <th>3836</th>
                <th>3706</th>
                <th>3515</th>
                <th>3281</th>
                <th>3087</th>
                <th>2884</th>
                <th>2673</th>
                <th>2479</th>
            </tr>
            <tr>
                <th>test</th>
                <th>471</th>
                <th>500</th>
                <th>539</th>
                <th>602</th>
                <th>658</th>
                <th>721</th>
                <th>787</th>
                <th>858</th>
                <th>938</th>
                <th>938</th>
                <th>938</th>
                <th>938</th>
                <th>938</th>
                <th>899</th>
                <th>866</th>
                <th>826</th>
                <th>767</th>
                <th>715</th>
                <th>670</th>
                <th>622</th>
                <th>579</th>
            </tr>
            <tr>
                <th rowspan=2>ModernBERT</th>
                <th>train</th>
                <th>1956</th>
                <th>2129</th>
                <th>2318</th>
                <th>2516</th>
                <th>2721</th>
                <th>2996</th>
                <th>3268</th>
                <th>3654</th>
                <th>3967</th>
                <th>3967</th>
                <th>3967</th>
                <th>3967</th>
                <th>3965</th>
                <th>3814</th>
                <th>3694</th>
                <th>3508</th>
                <th>3283</th>
                <th>3074</th>
                <th>2873</th>
                <th>2666</th>
                <th>2461</th>
            </tr>
            <tr>
                <th>test</th>
                <th>475</th>
                <th>510</th>
                <th>554</th>
                <th>610</th>
                <th>663</th>
                <th>735</th>
                <th>794</th>
                <th>870</th>
                <th>938</th>
                <th>938</th>
                <th>938</th>
                <th>938</th>
                <th>938</th>
                <th>898</th>
                <th>866</th>
                <th>823</th>
                <th>765</th>
                <th>710</th>
                <th>662</th>
                <th>620</th>
                <th>580</th>
            </tr>
        </tbody>
    </table>
</details>

I used both a linear probe and a non-linear probe passing through a ReLU layer, testing on a variety of hyperparameters. Here are some sample results (token 0 is the first token position of the verb):

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid path="assets/img/blog/probing/bert_linear_16.png" class="img-fluid rounded z-depth-1" zoomable=true %}
    </div>
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid path="assets/img/blog/probing/bert_linear_32.png" class="img-fluid rounded z-depth-1" zoomable=true %}
    </div>
</div>
<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid path="assets/img/blog/probing/bert_multi_16.png" class="img-fluid rounded z-depth-1" zoomable=true %}
    </div>
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid path="assets/img/blog/probing/bert_multi_32.png" class="img-fluid rounded z-depth-1" zoomable=true %}
    </div>
</div>
<div class="caption">
    Results from probing BERT-large with learning rate of 0.01 and batch size of 16. The first two use linear probes and bottom two use non-linear probes. Left results are trained for 16 epochs while the right results are trained for 32.
</div>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid path="assets/img/blog/probing/mbert_linear_16.png" class="img-fluid rounded z-depth-1" zoomable=true %}
    </div>
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid path="assets/img/blog/probing/mbert_linear_32.png" class="img-fluid rounded z-depth-1" zoomable=true %}
    </div>
</div>
<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid path="assets/img/blog/probing/mbert_multi_16.png" class="img-fluid rounded z-depth-1" zoomable=true %}
    </div>
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid path="assets/img/blog/probing/mbert_multi_32.png" class="img-fluid rounded z-depth-1" zoomable=true %}
    </div>
</div>
<div class="caption">
    Results from probing ModernBERT with learning rate of 0.001 and batch size of 8. The first two use linear probes and bottom two use non-linear probes. Left results are trained for 16 epochs while the right results are trained for 32.
</div>

In general, the probe results are all over the place and even the best-performing probes only do slightly better than chance. This isn't all surprising, since previous experiments showed that embeddings alone aren't good inputs for finding aspect. At the same time, there are two important findings:

**Finding 1: Aspect is strongly correlated with the token before the verb.** Although we expect the verb token to capture the best representation, the embeddings in the -1 position also performs quite well. In fact, the -1 position often performs better than the +1 position, even though the +1 position is sometimes part of the verb. 
- difference in input data size; still only the ±1 positions show any significant difference
- while +1 position is sometimes part of the verb, -1 position (never part of the verb) actually performs better. Suggests that sometimes self attention puts information in unexpected places.
- much more pronounced at higher epochs; potentially more true?

**Finding 2: Intermediate hidden layers seem to capture better representations.**
- verifies my hypothesis from before, initial neurospect results are poor because I'm only using output embeddings
- no clear picture on which layer is the best performing, more tests needed.

**Next Steps: Fuzzily Investigate Parts of Speech.**
- this experiment verifies that the event verb is highly important when looking at aspect, but doesn't rule out other parts of speech.
- however, we also know that it's not exactly the token position that matters, but also ones right before. 
- it's possible that different parts of speech will light up at different layers, painting a more accurate picture of how LLMs process aspect.

<br>

### Takeaways for Your Probing Experiments
1. Hypothesize about model behavior to focus your lens. If I didn't think about verb positions my data would not look clear at all.
2. Find the easiest experiment to run and build from there. I wanted to verify my ideas about intermediate layers and token positions. Now that I know, I can test sentences with multiple events and use PoS. Also, I know to test fuzzily.
3. Run multiple versions of the same experiment. Probing is an art, not a science. I wouldn't have figured out my gradient issue if I just had one heatmap to look at (would have concluded that BERT doesn't capture aspect). If I didn't tweak my hyperparameters, I wouldn't have as clear of findings as I do. 
4. Sometimes features are non-linear, so it's always smart to test with different types of probes. However, once your probes get too complex you should wonder if you're hallucinating.